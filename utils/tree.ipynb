{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Larger objects within clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of objects in the scene: 13\n",
      "\n",
      "Class labels for all objects in the scene:\n",
      "Object 0: multi_seat_sofa (Class Index: 16)\n",
      "Object 1: coffee_table (Class Index: 6)\n",
      "Object 2: armchair (Class Index: 0)\n",
      "Object 3: corner_side_table (Class Index: 8)\n",
      "Object 4: pendant_lamp (Class Index: 17)\n",
      "Object 5: dining_chair (Class Index: 10)\n",
      "Object 6: dining_chair (Class Index: 10)\n",
      "Object 7: dining_chair (Class Index: 10)\n",
      "Object 8: dining_table (Class Index: 11)\n",
      "Object 9: pendant_lamp (Class Index: 17)\n",
      "Object 10: console_table (Class Index: 7)\n",
      "Object 11: dining_chair (Class Index: 10)\n",
      "Object 12: stool (Class Index: 20)\n",
      "\n",
      "Clustering results using GMM:\n",
      "\n",
      "Cluster 1 contains the following objects:\n",
      "  Object 0: multi_seat_sofa\n",
      "  Object 1: coffee_table\n",
      "  Object 2: armchair\n",
      "  Object 4: pendant_lamp\n",
      "  Object 12: stool\n",
      "\n",
      "Cluster 0 contains the following objects:\n",
      "  Object 3: corner_side_table\n",
      "  Object 5: dining_chair\n",
      "  Object 6: dining_chair\n",
      "  Object 7: dining_chair\n",
      "  Object 8: dining_table\n",
      "  Object 9: pendant_lamp\n",
      "  Object 10: console_table\n",
      "  Object 11: dining_chair\n",
      "\n",
      "Detecting outliers using Elliptic Envelope...\n",
      "\n",
      "Outliers detected at indices: [ 3 12]\n",
      "Elliptic Envelope decision scores: [ 107.24919094  112.57784983  108.81951267   -1.85531323   69.67105536\n",
      "  113.703244    113.61268152  113.69634248  112.51594199    7.42124995\n",
      "  107.35857582  112.43200939 -271.9716426 ]\n",
      "[[1.4002417  0.4399975 ]\n",
      " [0.37961048 0.396937  ]\n",
      " [0.40990734 0.404042  ]\n",
      " [0.41999906 0.313436  ]\n",
      " [0.37032422 0.2949055 ]\n",
      " [0.2258811  0.4303595 ]\n",
      " [0.2258811  0.4303595 ]\n",
      " [0.2258811  0.4303595 ]\n",
      " [0.5500097  0.4346    ]\n",
      " [0.17074493 0.51308185]\n",
      " [0.6030077  0.5001644 ]\n",
      " [0.2258811  0.4303595 ]\n",
      " [0.2215995  0.1956395 ]]\n",
      "[0.6161029  0.15068145 0.16561979 0.13164283 0.10921066 0.09721008\n",
      " 0.09721008 0.09721008 0.23903422 0.08760612 0.30160296 0.09721008\n",
      " 0.04335362]\n",
      "\n",
      "Largest objects in each cluster by size:\n",
      "Cluster 0: Largest Object Index 10 with Area 0.30160295963287354 and Label 'console_table'\n",
      "Cluster 1: Largest Object Index 0 with Area 0.6161028742790222 and Label 'multi_seat_sofa'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gauravr/anaconda3/envs/physgen/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:187: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-5.870283065003756 > -40.280113148001455). You may want to try with a higher value of support_fraction (current value: 0.692).\n",
      "  warnings.warn(\n",
      "/home/gauravr/anaconda3/envs/physgen/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:187: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-5.982659322280981 > -42.617844626450328). You may want to try with a higher value of support_fraction (current value: 0.692).\n",
      "  warnings.warn(\n",
      "/home/gauravr/anaconda3/envs/physgen/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:187: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-5.870283065003756 > -40.280113148001455). You may want to try with a higher value of support_fraction (current value: 0.692).\n",
      "  warnings.warn(\n",
      "/home/gauravr/anaconda3/envs/physgen/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:187: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-5.982659322280981 > -42.617844626450328). You may want to try with a higher value of support_fraction (current value: 0.692).\n",
      "  warnings.warn(\n",
      "/home/gauravr/anaconda3/envs/physgen/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:187: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-5.870283065003756 > -40.280113148001455). You may want to try with a higher value of support_fraction (current value: 0.692).\n",
      "  warnings.warn(\n",
      "/home/gauravr/anaconda3/envs/physgen/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:187: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-5.982659322280981 > -39.244519731485788). You may want to try with a higher value of support_fraction (current value: 0.692).\n",
      "  warnings.warn(\n",
      "/home/gauravr/anaconda3/envs/physgen/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:187: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-5.982659322280981 > -42.617844626450328). You may want to try with a higher value of support_fraction (current value: 0.692).\n",
      "  warnings.warn(\n",
      "/home/gauravr/anaconda3/envs/physgen/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:187: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-4.986665760056301 > -39.957921357558334). You may want to try with a higher value of support_fraction (current value: 0.692).\n",
      "  warnings.warn(\n",
      "/home/gauravr/anaconda3/envs/physgen/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:187: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-4.986665760056301 > -39.957921357558334). You may want to try with a higher value of support_fraction (current value: 0.692).\n",
      "  warnings.warn(\n",
      "/home/gauravr/anaconda3/envs/physgen/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:187: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-4.986665760056301 > -39.957921357558334). You may want to try with a higher value of support_fraction (current value: 0.692).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "\n",
    "scene_synthesis_path = 'path_to_scene_synthesis'\n",
    "\n",
    "sys.path.append(os.path.dirname(scene_synthesis_path))\n",
    "sys.path.append(scene_synthesis_path)\n",
    "\n",
    "from scene_synthesis.datasets.threed_front import ThreedFront\n",
    "from scene_synthesis.datasets.threed_future_dataset import ThreedFutureDataset\n",
    "from scene_synthesis.utils import get_textured_objects, get_floor_plan\n",
    "from utils import export_scene, floor_plan_from_scene, render_scene_from_bbox_params, render_to_folder, render\n",
    "\n",
    "class_labels_dining = [\n",
    "    \"armchair\", \"bookshelf\", \"cabinet\", \"ceiling_lamp\", \n",
    "    \"chaise_longue_sofa\", \"chinese_chair\", \"coffee_table\", \n",
    "    \"console_table\", \"corner_side_table\", \"desk\", \"dining_chair\", \n",
    "    \"dining_table\", \"l_shaped_sofa\", \"lazy_sofa\", \"lounge_chair\", \n",
    "    \"loveseat_sofa\", \"multi_seat_sofa\", \"pendant_lamp\", \n",
    "    \"round_end_table\", \"shelf\", \"stool\", \"tv_stand\", \n",
    "    \"wardrobe\", \"wine_cabinet\", \"start\", \"end\"\n",
    "]\n",
    "\n",
    "class_labels_bedroom = [\"armchair\", \"bookshelf\", \"cabinet\", \"ceiling_lamp\", \"chair\", \n",
    "                        \"children_cabinet\", \"coffee_table\", \"desk\", \"double_bed\", \n",
    "                        \"dressing_chair\", \"dressing_table\", \"kids_bed\", \"nightstand\", \n",
    "                        \"pendant_lamp\", \"shelf\", \"single_bed\", \"sofa\", \"stool\", \"table\", \n",
    "                        \"tv_stand\", \"wardrobe\", \"start\", \"end\"]\n",
    "\n",
    "def fetch_scene_id(all_scene_paths, scene_id):\n",
    "    for scene in all_scene_paths:\n",
    "        if scene_id in scene:\n",
    "            return scene\n",
    "    return None\n",
    "\n",
    "def load_attributes_from_npz(scene_path, attrib_list=[]):\n",
    "    scene = np.load(os.path.join(scene_path, 'boxes.npz'))\n",
    "    if not attrib_list:\n",
    "        attrib_list = list(scene.keys())\n",
    "    vals = {}\n",
    "    for attrib in attrib_list:\n",
    "        vals[attrib] = scene[attrib]\n",
    "    return vals\n",
    "\n",
    "def calculate_giou(bbox1, bbox2):\n",
    "    def area(bbox):\n",
    "        return max(0, bbox[2] - bbox[0]) * max(0, bbox[3] - bbox[1])\n",
    "\n",
    "    xA = max(bbox1[0], bbox2[0])\n",
    "    yA = max(bbox1[1], bbox2[1])\n",
    "    xB = min(bbox1[2], bbox2[2])\n",
    "    yB = min(bbox1[3], bbox2[3])\n",
    "    intersection = area([xA, yA, xB, yB])\n",
    "    \n",
    "    union = area(bbox1) + area(bbox2) - intersection\n",
    "    \n",
    "    iou = intersection / union if union > 0 else 0\n",
    "    \n",
    "    xC = min(bbox1[0], bbox2[0])\n",
    "    yC = min(bbox1[1], bbox2[1])\n",
    "    xD = max(bbox1[2], bbox2[2])\n",
    "    yD = max(bbox1[3], bbox2[3])\n",
    "    enclosing_area = area([xC, yC, xD, yD])\n",
    "    \n",
    "    giou = iou - ((enclosing_area - union) / enclosing_area)\n",
    "    return giou\n",
    "\n",
    "def dist_matrix(translations, sizes, lambda_value=0.02):\n",
    "    num_boxes = len(translations)\n",
    "    distance_matrix = np.zeros((num_boxes, num_boxes))\n",
    "    \n",
    "    for i in range(num_boxes):\n",
    "        for j in range(num_boxes):\n",
    "            if i != j:\n",
    "                bbox_i = [\n",
    "                    translations[i][0] - sizes[i][0] / 2,\n",
    "                    translations[i][1] - sizes[i][1] / 2,\n",
    "                    translations[i][0] + sizes[i][0] / 2,\n",
    "                    translations[i][1] + sizes[i][1] / 2\n",
    "                ]\n",
    "                \n",
    "                bbox_j = [\n",
    "                    translations[j][0] - sizes[j][0] / 2,\n",
    "                    translations[j][1] - sizes[j][1] / 2,\n",
    "                    translations[j][0] + sizes[j][0] / 2,\n",
    "                    translations[j][1] + sizes[j][1] / 2\n",
    "                ]\n",
    "                \n",
    "                center_i = (translations[i][0], translations[i][1])\n",
    "                center_j = (translations[j][0], translations[j][1])\n",
    "                euclidean_distance = np.linalg.norm(np.array(center_i) - np.array(center_j))\n",
    "                \n",
    "                giou = calculate_giou(bbox_i, bbox_j)\n",
    "                \n",
    "                distance_matrix[i, j] = euclidean_distance + lambda_value * (1 - giou)\n",
    "\n",
    "    return distance_matrix\n",
    "\n",
    "def extract_features_from_bboxes(attrib):\n",
    "    class_labels = attrib[\"class_labels\"].argmax(-1)\n",
    "    translations = attrib[\"translations\"]\n",
    "    sizes = attrib[\"sizes\"]\n",
    "    angles = np.squeeze(attrib[\"angles\"])\n",
    "    \n",
    "    features_translations = translations[:,:2]\n",
    "    features_sizes = sizes[:,:2]\n",
    "    return features_translations, features_sizes\n",
    "\n",
    "def cluster_scene_objects_gmm(distance_matrix, n_components=5):\n",
    "    gmm = GaussianMixture(n_components=n_components, covariance_type='full')\n",
    "    cluster_labels = gmm.fit_predict(distance_matrix)\n",
    "    return cluster_labels\n",
    "\n",
    "def detect_outliers_elliptic_envelope(features, contamination=0.1):\n",
    "    scaler = StandardScaler()\n",
    "    features_std = scaler.fit_transform(features)\n",
    "\n",
    "    ee = EllipticEnvelope(contamination=contamination)\n",
    "    ee.fit(features_std)\n",
    "    \n",
    "    outlier_pred = ee.predict(features_std)\n",
    "    \n",
    "    outliers = np.where(outlier_pred == -1)[0]\n",
    "    \n",
    "    decision_scores = ee.decision_function(features_std)\n",
    "    \n",
    "    return outliers, decision_scores\n",
    "\n",
    "def find_largest_object_in_clusters(features_sizes, cluster_labels, class_labels):\n",
    "    cluster_largest_objects = {}\n",
    "    print(features_sizes)\n",
    "    areas = np.prod(features_sizes, axis=1)\n",
    "    print(areas)\n",
    "    unique_clusters = np.unique(cluster_labels)\n",
    "    for cluster in unique_clusters:\n",
    "        cluster_indices = np.where(cluster_labels == cluster)[0]\n",
    "        cluster_areas = areas[cluster_indices]\n",
    "        \n",
    "        largest_index_within_cluster = cluster_indices[np.argmax(cluster_areas)]\n",
    "        largest_class_label = class_labels[largest_index_within_cluster]\n",
    "        cluster_largest_objects[cluster] = {\n",
    "            \"index\": largest_index_within_cluster,\n",
    "            \"area\": cluster_areas.max(),\n",
    "            \"label\": largest_class_label\n",
    "        }\n",
    "    \n",
    "    return cluster_largest_objects\n",
    "\n",
    "def main(scene_id):\n",
    "    processed_path = '/home/gauravr/Desktop/IFA/code/preprocessed/3d_front_npz/'\n",
    "    if not os.path.exists(processed_path):\n",
    "        print(f\"Processed path {processed_path} does not exist.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    all_paths = [os.path.join(processed_path, dir_) for dir_ in os.listdir(processed_path)]\n",
    "    \n",
    "    all_scenes = []\n",
    "    for path_ in all_paths:\n",
    "        if os.path.isdir(path_):\n",
    "            all_scenes.extend([os.path.join(path_, x) for x in os.listdir(path_) if os.path.isdir(os.path.join(path_, x))])\n",
    "    \n",
    "    scene_path = fetch_scene_id(all_scenes, scene_id)\n",
    "    if scene_path is None:\n",
    "        print(f\"Scene ID {scene_id} not found.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    required_attribs = ['class_labels', 'translations', 'sizes', 'angles']\n",
    "    attribs = load_attributes_from_npz(scene_path, required_attribs)\n",
    "    \n",
    "    for attrib in required_attribs:\n",
    "        if attrib not in attribs:\n",
    "            print(f\"Attribute '{attrib}' not found in the NPZ file.\")\n",
    "            sys.exit(1)\n",
    "    \n",
    "    features_translations, features_sizes = extract_features_from_bboxes(attribs)\n",
    "\n",
    "    total_objects = attribs['class_labels'].shape[0]\n",
    "    print(f\"Total number of objects in the scene: {total_objects}\")\n",
    "\n",
    "    distance_matrix = dist_matrix(features_translations, features_sizes)\n",
    "\n",
    "    n_components = min(2, total_objects)\n",
    "    cluster_labels = cluster_scene_objects_gmm(distance_matrix, n_components=n_components)\n",
    "\n",
    "    class_labels = attribs['class_labels'].argmax(-1)\n",
    "\n",
    "    print(\"\\nClass labels for all objects in the scene:\")\n",
    "    for i, label_idx in enumerate(class_labels):\n",
    "        print(f\"Object {i}: {class_labels_dining[label_idx]} (Class Index: {label_idx})\")\n",
    "\n",
    "    print(f\"\\nClustering results using GMM:\")\n",
    "    cluster_dict = {}\n",
    "\n",
    "    for idx, label in enumerate(cluster_labels):\n",
    "        if label not in cluster_dict:\n",
    "            cluster_dict[label] = []\n",
    "        cluster_dict[label].append(idx)\n",
    "\n",
    "    # Print objects in each cluster with labels\n",
    "    for label, indices in cluster_dict.items():\n",
    "        print(f\"\\nCluster {label} contains the following objects:\")\n",
    "        for idx in indices:\n",
    "            print(f\"  Object {idx}: {class_labels_dining[class_labels[idx]]}\")\n",
    "\n",
    "    print(\"\\nDetecting outliers using Elliptic Envelope...\")\n",
    "    features = np.hstack((features_translations, features_sizes))\n",
    "    outliers, decision_scores = detect_outliers_elliptic_envelope(features)\n",
    "    \n",
    "    print(f\"\\nOutliers detected at indices: {outliers}\")\n",
    "    print(f\"Elliptic Envelope decision scores: {decision_scores}\")\n",
    "\n",
    "    cluster_largest_objects = find_largest_object_in_clusters(features_sizes, cluster_labels, class_labels)\n",
    "\n",
    "    print(\"\\nLargest objects in each cluster by size:\")\n",
    "    for cluster, info in cluster_largest_objects.items():\n",
    "        label_name = class_labels_dining[info['label']]\n",
    "        print(f\"Cluster {cluster}: Largest Object Index {info['index']} with Area {info['area']} and Label '{label_name}'\")\n",
    "\n",
    "if __name__ == \"__main__\":    \n",
    "    main(scene_id='LivingDiningRoom-941')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate tree within clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of objects in the scene: 16\n",
      "\n",
      "Class labels for all objects in the scene:\n",
      "Object 0: multi_seat_sofa (Class Index: 16)\n",
      "Object 1: loveseat_sofa (Class Index: 15)\n",
      "Object 2: pendant_lamp (Class Index: 17)\n",
      "Object 3: tv_stand (Class Index: 21)\n",
      "Object 4: bookshelf (Class Index: 1)\n",
      "Object 5: coffee_table (Class Index: 6)\n",
      "Object 6: dining_table (Class Index: 11)\n",
      "Object 7: dining_chair (Class Index: 10)\n",
      "Object 8: dining_chair (Class Index: 10)\n",
      "Object 9: dining_chair (Class Index: 10)\n",
      "Object 10: dining_chair (Class Index: 10)\n",
      "Object 11: dining_chair (Class Index: 10)\n",
      "Object 12: dining_chair (Class Index: 10)\n",
      "Object 13: shelf (Class Index: 19)\n",
      "Object 14: bookshelf (Class Index: 1)\n",
      "Object 15: pendant_lamp (Class Index: 17)\n",
      "\n",
      "Clustering results using GMM:\n",
      "\n",
      "Cluster 1 contains the following objects:\n",
      "  Object 0: multi_seat_sofa\n",
      "  Object 1: loveseat_sofa\n",
      "  Object 2: pendant_lamp\n",
      "  Object 3: tv_stand\n",
      "  Object 5: coffee_table\n",
      "  Object 14: bookshelf\n",
      "\n",
      "Cluster 0 contains the following objects:\n",
      "  Object 4: bookshelf\n",
      "  Object 6: dining_table\n",
      "  Object 7: dining_chair\n",
      "  Object 8: dining_chair\n",
      "  Object 9: dining_chair\n",
      "  Object 10: dining_chair\n",
      "  Object 11: dining_chair\n",
      "  Object 12: dining_chair\n",
      "  Object 13: shelf\n",
      "  Object 15: pendant_lamp\n",
      "\n",
      "Detecting outliers using Elliptic Envelope...\n",
      "\n",
      "Outliers detected at indices: [ 2 15]\n",
      "Elliptic Envelope decision scores: [ 1.20604336e+11  1.20604336e+11 -1.53760242e+11  1.20604336e+11\n",
      "  1.20604336e+11  1.20604336e+11  1.20604336e+11  1.20604336e+11\n",
      "  1.20604336e+11  1.20604336e+11  1.20604336e+11  1.20604336e+11\n",
      "  1.20604336e+11  1.20603852e+11  1.20604336e+11 -1.20603857e+11]\n",
      "\n",
      "Largest objects in each cluster by size:\n",
      "Cluster 0: Largest Object Index 6 with Area 0.3464892506599426 and Label 'dining_table'\n",
      "Cluster 1: Largest Object Index 0 with Area 0.4568370580673218 and Label 'multi_seat_sofa'\n",
      "\n",
      "Cluster 0 Tree Structure:\n",
      "Root: Object 6 (dining_table)\n",
      "  └─ Child Object 4 (bookshelf)\n",
      "  └─ Child Object 7 (dining_chair)\n",
      "  └─ Child Object 8 (dining_chair)\n",
      "  └─ Child Object 9 (dining_chair)\n",
      "  └─ Child Object 10 (dining_chair)\n",
      "  └─ Child Object 11 (dining_chair)\n",
      "  └─ Child Object 12 (dining_chair)\n",
      "  └─ Child Object 13 (shelf)\n",
      "  └─ Child Object 15 (pendant_lamp)\n",
      "\n",
      "Cluster 1 Tree Structure:\n",
      "Root: Object 0 (multi_seat_sofa)\n",
      "  └─ Child Object 1 (loveseat_sofa)\n",
      "  └─ Child Object 2 (pendant_lamp)\n",
      "  └─ Child Object 3 (tv_stand)\n",
      "  └─ Child Object 5 (coffee_table)\n",
      "  └─ Child Object 14 (bookshelf)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "\n",
    "scene_synthesis_path = 'path_to_scene_synthesis'\n",
    "\n",
    "sys.path.append(os.path.dirname(scene_synthesis_path))\n",
    "sys.path.append(scene_synthesis_path)\n",
    "\n",
    "from scene_synthesis.datasets.threed_front import ThreedFront\n",
    "from scene_synthesis.datasets.threed_future_dataset import ThreedFutureDataset\n",
    "from scene_synthesis.utils import get_textured_objects, get_floor_plan\n",
    "from utils import export_scene, floor_plan_from_scene, render_scene_from_bbox_params, render_to_folder, render\n",
    "\n",
    "class_labels_dining = [\n",
    "    \"armchair\", \"bookshelf\", \"cabinet\", \"ceiling_lamp\", \n",
    "    \"chaise_longue_sofa\", \"chinese_chair\", \"coffee_table\", \n",
    "    \"console_table\", \"corner_side_table\", \"desk\", \"dining_chair\", \n",
    "    \"dining_table\", \"l_shaped_sofa\", \"lazy_sofa\", \"lounge_chair\", \n",
    "    \"loveseat_sofa\", \"multi_seat_sofa\", \"pendant_lamp\", \n",
    "    \"round_end_table\", \"shelf\", \"stool\", \"tv_stand\", \n",
    "    \"wardrobe\", \"wine_cabinet\", \"start\", \"end\"\n",
    "]\n",
    "\n",
    "class_labels_bedroom = [\"armchair\", \"bookshelf\", \"cabinet\", \"ceiling_lamp\", \"chair\", \n",
    "                        \"children_cabinet\", \"coffee_table\", \"desk\", \"double_bed\", \n",
    "                        \"dressing_chair\", \"dressing_table\", \"kids_bed\", \"nightstand\", \n",
    "                        \"pendant_lamp\", \"shelf\", \"single_bed\", \"sofa\", \"stool\", \"table\", \n",
    "                        \"tv_stand\", \"wardrobe\", \"start\", \"end\"]\n",
    "\n",
    "def fetch_scene_id(all_scene_paths, scene_id):\n",
    "    for scene in all_scene_paths:\n",
    "        if scene_id in scene:\n",
    "            return scene\n",
    "    return None\n",
    "\n",
    "def load_attributes_from_npz(scene_path, attrib_list=[]):\n",
    "    scene = np.load(os.path.join(scene_path, 'boxes.npz'))\n",
    "    if not attrib_list:\n",
    "        attrib_list = list(scene.keys())\n",
    "    vals = {}\n",
    "    for attrib in attrib_list:\n",
    "        vals[attrib] = scene[attrib]\n",
    "    return vals\n",
    "\n",
    "def calculate_giou(bbox1, bbox2):\n",
    "    def area(bbox):\n",
    "        return max(0, bbox[2] - bbox[0]) * max(0, bbox[3] - bbox[1])\n",
    "\n",
    "    xA = max(bbox1[0], bbox2[0])\n",
    "    yA = max(bbox1[1], bbox2[1])\n",
    "    xB = min(bbox1[2], bbox2[2])\n",
    "    yB = min(bbox1[3], bbox2[3])\n",
    "    intersection = area([xA, yA, xB, yB])\n",
    "    \n",
    "    union = area(bbox1) + area(bbox2) - intersection\n",
    "    \n",
    "    iou = intersection / union if union > 0 else 0\n",
    "    \n",
    "    xC = min(bbox1[0], bbox2[0])\n",
    "    yC = min(bbox1[1], bbox2[1])\n",
    "    xD = max(bbox1[2], bbox2[2])\n",
    "    yD = max(bbox1[3], bbox2[3])\n",
    "    enclosing_area = area([xC, yC, xD, yD])\n",
    "    \n",
    "    giou = iou - ((enclosing_area - union) / enclosing_area)\n",
    "    return giou\n",
    "\n",
    "def dist_matrix(translations, sizes, lambda_value=0.02):\n",
    "    num_boxes = len(translations)\n",
    "    distance_matrix = np.zeros((num_boxes, num_boxes))\n",
    "    \n",
    "    for i in range(num_boxes):\n",
    "        for j in range(num_boxes):\n",
    "            if i != j:\n",
    "                bbox_i = [\n",
    "                    translations[i][0] - sizes[i][0] / 2,\n",
    "                    translations[i][1] - sizes[i][1] / 2,\n",
    "                    translations[i][0] + sizes[i][0] / 2,\n",
    "                    translations[i][1] + sizes[i][1] / 2\n",
    "                ]\n",
    "                \n",
    "                bbox_j = [\n",
    "                    translations[j][0] - sizes[j][0] / 2,\n",
    "                    translations[j][1] - sizes[j][1] / 2,\n",
    "                    translations[j][0] + sizes[j][0] / 2,\n",
    "                    translations[j][1] + sizes[j][1] / 2\n",
    "                ]\n",
    "                \n",
    "                center_i = (translations[i][0], translations[i][1])\n",
    "                center_j = (translations[j][0], translations[j][1])\n",
    "                euclidean_distance = np.linalg.norm(np.array(center_i) - np.array(center_j))\n",
    "                \n",
    "                giou = calculate_giou(bbox_i, bbox_j)\n",
    "                \n",
    "                distance_matrix[i, j] = euclidean_distance + lambda_value * (1 - giou)\n",
    "\n",
    "    return distance_matrix\n",
    "\n",
    "def extract_features_from_bboxes(attrib):\n",
    "    class_labels = attrib[\"class_labels\"].argmax(-1)\n",
    "    translations = attrib[\"translations\"]\n",
    "    sizes = attrib[\"sizes\"]\n",
    "    angles = np.squeeze(attrib[\"angles\"])\n",
    "    \n",
    "    features_translations = translations[:,:2]\n",
    "    features_sizes = sizes[:,:2]\n",
    "    return features_translations, features_sizes\n",
    "\n",
    "def cluster_scene_objects_gmm(distance_matrix, n_components=5):\n",
    "    gmm = GaussianMixture(n_components=n_components, covariance_type='full')\n",
    "    cluster_labels = gmm.fit_predict(distance_matrix)\n",
    "    return cluster_labels\n",
    "\n",
    "def detect_outliers_elliptic_envelope(features, contamination=0.1):\n",
    "    scaler = StandardScaler()\n",
    "    features_std = scaler.fit_transform(features)\n",
    "\n",
    "    ee = EllipticEnvelope(contamination=contamination)\n",
    "    ee.fit(features_std)\n",
    "    \n",
    "    outlier_pred = ee.predict(features_std)\n",
    "    \n",
    "    outliers = np.where(outlier_pred == -1)[0]\n",
    "    \n",
    "    decision_scores = ee.decision_function(features_std)\n",
    "    \n",
    "    return outliers, decision_scores\n",
    "\n",
    "def find_largest_object_in_clusters(features_sizes, cluster_labels, class_labels):\n",
    "    cluster_largest_objects = {}\n",
    "    areas = np.prod(features_sizes, axis=1)\n",
    "    unique_clusters = np.unique(cluster_labels)\n",
    "    for cluster in unique_clusters:\n",
    "        cluster_indices = np.where(cluster_labels == cluster)[0]\n",
    "        cluster_areas = areas[cluster_indices]\n",
    "        \n",
    "        largest_index_within_cluster = cluster_indices[np.argmax(cluster_areas)]\n",
    "        largest_class_label = class_labels[largest_index_within_cluster]\n",
    "        cluster_largest_objects[cluster] = {\n",
    "            \"index\": largest_index_within_cluster,\n",
    "            \"area\": cluster_areas.max(),\n",
    "            \"label\": largest_class_label\n",
    "        }\n",
    "    \n",
    "    return cluster_largest_objects\n",
    "\n",
    "def build_tree(cluster_largest_objects, cluster_labels, features_sizes, class_labels, label_names):\n",
    "    cluster_trees = {}\n",
    "    for cluster, info in cluster_largest_objects.items():\n",
    "        root_label = label_names[info['label']]\n",
    "        root_index = info['index']\n",
    "        \n",
    "        # Initialize the root node of the tree with the largest object\n",
    "        tree = {root_index: {\"label\": root_label, \"children\": []}}\n",
    "        cluster_indices = np.where(cluster_labels == cluster)[0]\n",
    "        \n",
    "        for idx in cluster_indices:\n",
    "            if idx == root_index:\n",
    "                continue  # skip the root node\n",
    "            \n",
    "            child_label = label_names[class_labels[idx]]\n",
    "            \n",
    "            # Attach all items directly under the root node\n",
    "            tree[root_index][\"children\"].append({\"index\": idx, \"label\": child_label, \"children\": []})\n",
    "        \n",
    "        cluster_trees[cluster] = tree\n",
    "\n",
    "    return cluster_trees\n",
    "\n",
    "def main(scene_id):\n",
    "    processed_path = '/home/gauravr/Desktop/IFA/code/preprocessed/3d_front_npz/'\n",
    "    if not os.path.exists(processed_path):\n",
    "        print(f\"Processed path {processed_path} does not exist.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    all_paths = [os.path.join(processed_path, dir_) for dir_ in os.listdir(processed_path)]\n",
    "    \n",
    "    all_scenes = []\n",
    "    for path_ in all_paths:\n",
    "        if os.path.isdir(path_):\n",
    "            all_scenes.extend([os.path.join(path_, x) for x in os.listdir(path_) if os.path.isdir(os.path.join(path_, x))])\n",
    "    \n",
    "    scene_path = fetch_scene_id(all_scenes, scene_id)\n",
    "    if scene_path is None:\n",
    "        print(f\"Scene ID {scene_id} not found.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    required_attribs = ['class_labels', 'translations', 'sizes', 'angles']\n",
    "    attribs = load_attributes_from_npz(scene_path, required_attribs)\n",
    "    \n",
    "    for attrib in required_attribs:\n",
    "        if attrib not in attribs:\n",
    "            print(f\"Attribute '{attrib}' not found in the NPZ file.\")\n",
    "            sys.exit(1)\n",
    "    \n",
    "    features_translations, features_sizes = extract_features_from_bboxes(attribs)\n",
    "\n",
    "    total_objects = attribs['class_labels'].shape[0]\n",
    "    print(f\"Total number of objects in the scene: {total_objects}\")\n",
    "\n",
    "    distance_matrix = dist_matrix(features_translations, features_sizes)\n",
    "\n",
    "    n_components = min(2, total_objects)\n",
    "    cluster_labels = cluster_scene_objects_gmm(distance_matrix, n_components=n_components)\n",
    "\n",
    "    class_labels = attribs['class_labels'].argmax(-1)\n",
    "\n",
    "    print(\"\\nClass labels for all objects in the scene:\")\n",
    "    for i, label_idx in enumerate(class_labels):\n",
    "        print(f\"Object {i}: {class_labels_dining[label_idx]} (Class Index: {label_idx})\")\n",
    "\n",
    "    print(f\"\\nClustering results using GMM:\")\n",
    "    cluster_dict = {}\n",
    "\n",
    "    for idx, label in enumerate(cluster_labels):\n",
    "        if label not in cluster_dict:\n",
    "            cluster_dict[label] = []\n",
    "        cluster_dict[label].append(idx)\n",
    "\n",
    "    for label, indices in cluster_dict.items():\n",
    "        print(f\"\\nCluster {label} contains the following objects:\")\n",
    "        for idx in indices:\n",
    "            print(f\"  Object {idx}: {class_labels_dining[class_labels[idx]]}\")\n",
    "\n",
    "    print(\"\\nDetecting outliers using Elliptic Envelope...\")\n",
    "    features = np.hstack((features_translations, features_sizes))\n",
    "    outliers, decision_scores = detect_outliers_elliptic_envelope(features)\n",
    "    \n",
    "    print(f\"\\nOutliers detected at indices: {outliers}\")\n",
    "    print(f\"Elliptic Envelope decision scores: {decision_scores}\")\n",
    "\n",
    "    cluster_largest_objects = find_largest_object_in_clusters(features_sizes, cluster_labels, class_labels)\n",
    "\n",
    "    print(\"\\nLargest objects in each cluster by size:\")\n",
    "    for cluster, info in cluster_largest_objects.items():\n",
    "        label_name = class_labels_dining[info['label']]\n",
    "        print(f\"Cluster {cluster}: Largest Object Index {info['index']} with Area {info['area']} and Label '{label_name}'\")\n",
    "\n",
    "    cluster_trees = build_tree(cluster_largest_objects, cluster_labels, features_sizes, class_labels, class_labels_dining)\n",
    "\n",
    "    for cluster, tree in cluster_trees.items():\n",
    "        print(f\"\\nCluster {cluster} Tree Structure:\")\n",
    "        for node, data in tree.items():\n",
    "            print(f\"Root: Object {node} ({data['label']})\")\n",
    "            for child in data[\"children\"]:\n",
    "                print(f\"  └─ Child Object {child['index']} ({child['label']})\")\n",
    "\n",
    "if __name__ == \"__main__\":    \n",
    "    main(scene_id='LivingDiningRoom-8302')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of objects in the scene: 12\n",
      "\n",
      "Class labels for all objects in the scene:\n",
      "Object 0: wardrobe (Class Index: 20)\n",
      "Object 1: wardrobe (Class Index: 20)\n",
      "Object 2: single_bed (Class Index: 15)\n",
      "Object 3: nightstand (Class Index: 12)\n",
      "Object 4: nightstand (Class Index: 12)\n",
      "Object 5: cabinet (Class Index: 2)\n",
      "Object 6: chair (Class Index: 4)\n",
      "Object 7: chair (Class Index: 4)\n",
      "Object 8: table (Class Index: 18)\n",
      "Object 9: pendant_lamp (Class Index: 13)\n",
      "Object 10: pendant_lamp (Class Index: 13)\n",
      "Object 11: tv_stand (Class Index: 19)\n",
      "\n",
      "Clustering results using GMM:\n",
      "\n",
      "Cluster 1 contains the following objects:\n",
      "  Object 0: wardrobe\n",
      "  Object 2: single_bed\n",
      "  Object 3: nightstand\n",
      "  Object 4: nightstand\n",
      "  Object 5: cabinet\n",
      "  Object 7: chair\n",
      "  Object 8: table\n",
      "\n",
      "Cluster 0 contains the following objects:\n",
      "  Object 1: wardrobe\n",
      "  Object 6: chair\n",
      "  Object 9: pendant_lamp\n",
      "  Object 10: pendant_lamp\n",
      "  Object 11: tv_stand\n",
      "\n",
      "Detecting outliers using Elliptic Envelope...\n",
      "\n",
      "Outliers detected at indices: [ 9 10]\n",
      "Elliptic Envelope decision scores: [ 6.49899497e+13  6.49899497e+13  6.49899497e+13  6.49899497e+13\n",
      "  6.49899497e+13  6.49899497e+13  6.49899497e+13  6.49899497e+13\n",
      "  6.49899497e+13 -7.22111126e+12 -3.07794474e+13  6.49899497e+13]\n",
      "\n",
      "Largest objects in each cluster by size:\n",
      "Cluster 0: Largest Object Index 1 with Area 1.0800000429153442 and Label 'wardrobe'\n",
      "Cluster 1: Largest Object Index 0 with Area 1.0800000429153442 and Label 'wardrobe'\n",
      "\n",
      "Cluster 0 Tree Structure:\n",
      "Root: Object 1 (wardrobe)\n",
      "  └─ Child Object 6 (chair)\n",
      "  └─ Child Object 9 (pendant_lamp)\n",
      "  └─ Child Object 10 (pendant_lamp)\n",
      "  └─ Child Object 11 (tv_stand)\n",
      "\n",
      "Cluster 1 Tree Structure:\n",
      "Root: Object 0 (wardrobe)\n",
      "  └─ Child Object 2 (single_bed)\n",
      "  └─ Child Object 3 (nightstand)\n",
      "  └─ Child Object 4 (nightstand)\n",
      "  └─ Child Object 5 (cabinet)\n",
      "  └─ Child Object 7 (chair)\n",
      "  └─ Child Object 8 (table)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gauravr/anaconda3/envs/physgen/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:187: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-3.184751963521167 > -36.224537516639444). You may want to try with a higher value of support_fraction (current value: 0.750).\n",
      "  warnings.warn(\n",
      "/home/gauravr/anaconda3/envs/physgen/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:187: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-3.184751963521167 > -36.224537516639444). You may want to try with a higher value of support_fraction (current value: 0.750).\n",
      "  warnings.warn(\n",
      "/home/gauravr/anaconda3/envs/physgen/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:187: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-3.184751963521167 > -36.224537516639444). You may want to try with a higher value of support_fraction (current value: 0.750).\n",
      "  warnings.warn(\n",
      "/home/gauravr/anaconda3/envs/physgen/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:187: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-3.184751963521167 > -36.224537516639444). You may want to try with a higher value of support_fraction (current value: 0.750).\n",
      "  warnings.warn(\n",
      "/home/gauravr/anaconda3/envs/physgen/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:187: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-3.184751963521167 > -36.224537516639444). You may want to try with a higher value of support_fraction (current value: 0.750).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "\n",
    "scene_synthesis_path = 'path_to_scene_synthesis'\n",
    "\n",
    "sys.path.append(os.path.dirname(scene_synthesis_path))\n",
    "sys.path.append(scene_synthesis_path)\n",
    "\n",
    "from scene_synthesis.datasets.threed_front import ThreedFront\n",
    "from scene_synthesis.datasets.threed_future_dataset import ThreedFutureDataset\n",
    "from scene_synthesis.utils import get_textured_objects, get_floor_plan\n",
    "from utils import export_scene, floor_plan_from_scene, render_scene_from_bbox_params, render_to_folder, render\n",
    "\n",
    "class_labels_dining = [\n",
    "    \"armchair\", \"bookshelf\", \"cabinet\", \"ceiling_lamp\", \n",
    "    \"chaise_longue_sofa\", \"chinese_chair\", \"coffee_table\", \n",
    "    \"console_table\", \"corner_side_table\", \"desk\", \"dining_chair\", \n",
    "    \"dining_table\", \"l_shaped_sofa\", \"lazy_sofa\", \"lounge_chair\", \n",
    "    \"loveseat_sofa\", \"multi_seat_sofa\", \"pendant_lamp\", \n",
    "    \"round_end_table\", \"shelf\", \"stool\", \"tv_stand\", \n",
    "    \"wardrobe\", \"wine_cabinet\", \"start\", \"end\"]\n",
    "\n",
    "class_labels_bedroom = [\n",
    "    \"armchair\", \"bookshelf\", \"cabinet\", \"ceiling_lamp\", \"chair\", \n",
    "    \"children_cabinet\", \"coffee_table\", \"desk\", \"double_bed\", \n",
    "    \"dressing_chair\", \"dressing_table\", \"kids_bed\", \"nightstand\", \n",
    "    \"pendant_lamp\", \"shelf\", \"single_bed\", \"sofa\", \"stool\", \"table\", \n",
    "    \"tv_stand\", \"wardrobe\", \"start\", \"end\"]\n",
    "\n",
    "def fetch_scene_id(all_scene_paths, scene_id):\n",
    "    for scene in all_scene_paths:\n",
    "        if scene_id in scene:\n",
    "            return scene\n",
    "    return None\n",
    "\n",
    "def load_attributes_from_npz(scene_path, attrib_list=[]):\n",
    "    scene = np.load(os.path.join(scene_path, 'boxes.npz'))\n",
    "    if not attrib_list:\n",
    "        attrib_list = list(scene.keys())\n",
    "    vals = {}\n",
    "    for attrib in attrib_list:\n",
    "        vals[attrib] = scene[attrib]\n",
    "    return vals\n",
    "\n",
    "def calculate_giou(bbox1, bbox2):\n",
    "    def area(bbox):\n",
    "        return max(0, bbox[2] - bbox[0]) * max(0, bbox[3] - bbox[1])\n",
    "\n",
    "    xA = max(bbox1[0], bbox2[0])\n",
    "    yA = max(bbox1[1], bbox2[1])\n",
    "    xB = min(bbox1[2], bbox2[2])\n",
    "    yB = min(bbox1[3], bbox2[3])\n",
    "    intersection = area([xA, yA, xB, yB])\n",
    "    \n",
    "    union = area(bbox1) + area(bbox2) - intersection\n",
    "    \n",
    "    iou = intersection / union if union > 0 else 0\n",
    "    \n",
    "    xC = min(bbox1[0], bbox2[0])\n",
    "    yC = min(bbox1[1], bbox2[1])\n",
    "    xD = max(bbox1[2], bbox2[2])\n",
    "    yD = max(bbox1[3], bbox2[3])\n",
    "    enclosing_area = area([xC, yC, xD, yD])\n",
    "    \n",
    "    giou = iou - ((enclosing_area - union) / enclosing_area)\n",
    "    return giou\n",
    "\n",
    "def dist_matrix(translations, sizes, lambda_value=0.02):\n",
    "    num_boxes = len(translations)\n",
    "    distance_matrix = np.zeros((num_boxes, num_boxes))\n",
    "    \n",
    "    for i in range(num_boxes):\n",
    "        for j in range(num_boxes):\n",
    "            if i != j:\n",
    "                bbox_i = [\n",
    "                    translations[i][0] - sizes[i][0] / 2,\n",
    "                    translations[i][1] - sizes[i][1] / 2,\n",
    "                    translations[i][0] + sizes[i][0] / 2,\n",
    "                    translations[i][1] + sizes[i][1] / 2\n",
    "                ]\n",
    "                \n",
    "                bbox_j = [\n",
    "                    translations[j][0] - sizes[j][0] / 2,\n",
    "                    translations[j][1] - sizes[j][1] / 2,\n",
    "                    translations[j][0] + sizes[j][0] / 2,\n",
    "                    translations[j][1] + sizes[j][1] / 2\n",
    "                ]\n",
    "                \n",
    "                center_i = (translations[i][0], translations[i][1])\n",
    "                center_j = (translations[j][0], translations[j][1])\n",
    "                euclidean_distance = np.linalg.norm(np.array(center_i) - np.array(center_j))\n",
    "                \n",
    "                giou = calculate_giou(bbox_i, bbox_j)\n",
    "                \n",
    "                distance_matrix[i, j] = euclidean_distance + lambda_value * (1 - giou)\n",
    "\n",
    "    return distance_matrix\n",
    "\n",
    "def extract_features_from_bboxes(attrib):\n",
    "    class_labels = attrib[\"class_labels\"].argmax(-1)\n",
    "    translations = attrib[\"translations\"]\n",
    "    sizes = attrib[\"sizes\"]\n",
    "    angles = np.squeeze(attrib[\"angles\"])\n",
    "    \n",
    "    features_translations = translations[:,:2]\n",
    "    features_sizes = sizes[:,:2]\n",
    "    return features_translations, features_sizes\n",
    "\n",
    "def cluster_scene_objects_gmm(distance_matrix, n_components=5):\n",
    "    gmm = GaussianMixture(n_components=n_components, covariance_type='full')\n",
    "    cluster_labels = gmm.fit_predict(distance_matrix)\n",
    "    return cluster_labels\n",
    "\n",
    "def detect_outliers_elliptic_envelope(features, contamination=0.1):\n",
    "    scaler = StandardScaler()\n",
    "    features_std = scaler.fit_transform(features)\n",
    "\n",
    "    ee = EllipticEnvelope(contamination=contamination)\n",
    "    ee.fit(features_std)\n",
    "    \n",
    "    outlier_pred = ee.predict(features_std)\n",
    "    \n",
    "    outliers = np.where(outlier_pred == -1)[0]\n",
    "    \n",
    "    decision_scores = ee.decision_function(features_std)\n",
    "    \n",
    "    return outliers, decision_scores\n",
    "\n",
    "def find_largest_object_in_clusters(features_sizes, cluster_labels, class_labels):\n",
    "    cluster_largest_objects = {}\n",
    "    areas = np.prod(features_sizes, axis=1)\n",
    "    unique_clusters = np.unique(cluster_labels)\n",
    "    for cluster in unique_clusters:\n",
    "        cluster_indices = np.where(cluster_labels == cluster)[0]\n",
    "        cluster_areas = areas[cluster_indices]\n",
    "        \n",
    "        largest_index_within_cluster = cluster_indices[np.argmax(cluster_areas)]\n",
    "        largest_class_label = class_labels[largest_index_within_cluster]\n",
    "        cluster_largest_objects[cluster] = {\n",
    "            \"index\": largest_index_within_cluster,\n",
    "            \"area\": cluster_areas.max(),\n",
    "            \"label\": largest_class_label\n",
    "        }\n",
    "    \n",
    "    return cluster_largest_objects\n",
    "\n",
    "def build_tree(cluster_largest_objects, cluster_labels, features_sizes, class_labels, label_names):\n",
    "    cluster_trees = {}\n",
    "    for cluster, info in cluster_largest_objects.items():\n",
    "        root_label = label_names[info['label']]\n",
    "        root_index = info['index']\n",
    "        \n",
    "        # Initialize the root node of the tree with the largest object\n",
    "        tree = {root_index: {\"label\": root_label, \"children\": []}}\n",
    "        cluster_indices = np.where(cluster_labels == cluster)[0]\n",
    "        \n",
    "        for idx in cluster_indices:\n",
    "            if idx == root_index:\n",
    "                continue  # skip the root node\n",
    "            \n",
    "            child_label = label_names[class_labels[idx]]\n",
    "            \n",
    "            # Attach all items directly under the root node\n",
    "            tree[root_index][\"children\"].append({\"index\": idx, \"label\": child_label, \"children\": []})\n",
    "        \n",
    "        cluster_trees[cluster] = tree\n",
    "\n",
    "    return cluster_trees\n",
    "\n",
    "def main(scene_id):\n",
    "    processed_path = '/home/gauravr/Desktop/IFA/code/preprocessed/3d_front_npz/'\n",
    "    if not os.path.exists(processed_path):\n",
    "        print(f\"Processed path {processed_path} does not exist.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    all_paths = [os.path.join(processed_path, dir_) for dir_ in os.listdir(processed_path)]\n",
    "    \n",
    "    all_scenes = []\n",
    "    for path_ in all_paths:\n",
    "        if os.path.isdir(path_):\n",
    "            all_scenes.extend([os.path.join(path_, x) for x in os.listdir(path_) if os.path.isdir(os.path.join(path_, x))])\n",
    "    \n",
    "    scene_path = fetch_scene_id(all_scenes, scene_id)\n",
    "    if scene_path is None:\n",
    "        print(f\"Scene ID {scene_id} not found.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    required_attribs = ['class_labels', 'translations', 'sizes', 'angles']\n",
    "    attribs = load_attributes_from_npz(scene_path, required_attribs)\n",
    "    \n",
    "    for attrib in required_attribs:\n",
    "        if attrib not in attribs:\n",
    "            print(f\"Attribute '{attrib}' not found in the NPZ file.\")\n",
    "            sys.exit(1)\n",
    "    \n",
    "    features_translations, features_sizes = extract_features_from_bboxes(attribs)\n",
    "\n",
    "    total_objects = attribs['class_labels'].shape[0]\n",
    "    print(f\"Total number of objects in the scene: {total_objects}\")\n",
    "\n",
    "    distance_matrix = dist_matrix(features_translations, features_sizes)\n",
    "\n",
    "    n_components = min(2, total_objects)\n",
    "    cluster_labels = cluster_scene_objects_gmm(distance_matrix, n_components=n_components)\n",
    "\n",
    "    class_labels = attribs['class_labels'].argmax(-1)\n",
    "\n",
    "    print(\"\\nClass labels for all objects in the scene:\")\n",
    "    for i, label_idx in enumerate(class_labels):\n",
    "        print(f\"Object {i}: {class_labels_bedroom[label_idx]} (Class Index: {label_idx})\")\n",
    "\n",
    "    print(f\"\\nClustering results using GMM:\")\n",
    "    cluster_dict = {}\n",
    "\n",
    "    for idx, label in enumerate(cluster_labels):\n",
    "        if label not in cluster_dict:\n",
    "            cluster_dict[label] = []\n",
    "        cluster_dict[label].append(idx)\n",
    "\n",
    "    for label, indices in cluster_dict.items():\n",
    "        print(f\"\\nCluster {label} contains the following objects:\")\n",
    "        for idx in indices:\n",
    "            print(f\"  Object {idx}: {class_labels_bedroom[class_labels[idx]]}\")\n",
    "\n",
    "    print(\"\\nDetecting outliers using Elliptic Envelope...\")\n",
    "    features = np.hstack((features_translations, features_sizes))\n",
    "    outliers, decision_scores = detect_outliers_elliptic_envelope(features)\n",
    "    \n",
    "    print(f\"\\nOutliers detected at indices: {outliers}\")\n",
    "    print(f\"Elliptic Envelope decision scores: {decision_scores}\")\n",
    "\n",
    "    cluster_largest_objects = find_largest_object_in_clusters(features_sizes, cluster_labels, class_labels)\n",
    "\n",
    "    print(\"\\nLargest objects in each cluster by size:\")\n",
    "    for cluster, info in cluster_largest_objects.items():\n",
    "        label_name = class_labels_bedroom[info['label']]\n",
    "        print(f\"Cluster {cluster}: Largest Object Index {info['index']} with Area {info['area']} and Label '{label_name}'\")\n",
    "\n",
    "    cluster_trees = build_tree(cluster_largest_objects, cluster_labels, features_sizes, class_labels, class_labels_bedroom)\n",
    "\n",
    "    for cluster, tree in cluster_trees.items():\n",
    "        print(f\"\\nCluster {cluster} Tree Structure:\")\n",
    "        for node, data in tree.items():\n",
    "            print(f\"Root: Object {node} ({data['label']})\")\n",
    "            for child in data[\"children\"]:\n",
    "                print(f\"  └─ Child Object {child['index']} ({child['label']})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(scene_id='MasterBedroom-71933')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "physgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
