{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM spatial reasoning for object relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relation â†’ Higher weight value denotes strong relation and lower value denotes weak relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    \"armchair\": {\n",
    "        \"bookshelf\": 0.2, \"cabinet\": 0.3, \"ceiling_lamp\": 0.4, \"chaise_longue_sofa\": 0.5, \n",
    "        \"chinese_chair\": 0.3, \"coffee_table\": 0.6, \"console_table\": 0.4, \"corner_side_table\": 0.5, \n",
    "        \"desk\": 0.3, \"dining_chair\": 0.2, \"dining_table\": 0.2, \"l_shaped_sofa\": 0.5, \n",
    "        \"lazy_sofa\": 0.5, \"lounge_chair\": 0.7, \"loveseat_sofa\": 0.6, \"multi_seat_sofa\": 0.4, \n",
    "        \"pendent_lamp\": 0.8, \"round_end_table\": 0.6, \"shelf\": 0.3, \"stool\": 0.4, \"tv_stand\": 0.3, \n",
    "        \"wardrobe\": 0.2, \"wine_cabinet\": 0.1\n",
    "    },\n",
    "    \"bookshelf\": {\n",
    "        \"armchair\": 0.2, \"cabinet\": 0.5, \"ceiling_lamp\": 0.2, \"chaise_longue_sofa\": 0.3, \n",
    "        \"chinese_chair\": 0.3, \"coffee_table\": 0.3, \"console_table\": 0.3, \"corner_side_table\": 0.2, \n",
    "        \"desk\": 0.7, \"dining_chair\": 0.2, \"dining_table\": 0.1, \"l_shaped_sofa\": 0.4, \n",
    "        \"lazy_sofa\": 0.4, \"lounge_chair\": 0.3, \"loveseat_sofa\": 0.2, \"multi_seat_sofa\": 0.2, \n",
    "        \"pendant_lamp\": 0.4, \"round_end_table\": 0.3, \"shelf\": 0.6, \"stool\": 0.3, \"tv_stand\": 0.2, \n",
    "        \"wardrobe\": 0.3, \"wine_cabinet\": 0.2\n",
    "    },\n",
    "    \"cabinet\": {\n",
    "        \"armchair\": 0.3, \"bookshelf\": 0.5, \"ceiling_lamp\": 0.2, \"chaise_longue_sofa\": 0.3, \n",
    "        \"chinese_chair\": 0.2, \"coffee_table\": 0.2, \"console_table\": 0.3, \"corner_side_table\": 0.2, \n",
    "        \"desk\": 0.4, \"dining_chair\": 0.2, \"dining_table\": 0.1, \"l_shaped_sofa\": 0.3, \n",
    "        \"lazy_sofa\": 0.3, \"lounge_chair\": 0.4, \"loveseat_sofa\": 0.3, \"multi_seat_sofa\": 0.2, \n",
    "        \"pendant_lamp\": 0.3, \"round_end_table\": 0.2, \"shelf\": 0.8, \"stool\": 0.3, \"tv_stand\": 0.4, \n",
    "        \"wardrobe\": 0.9, \"wine_cabinet\": 0.5\n",
    "    },\n",
    "    \"ceiling_lamp\": {\n",
    "        \"armchair\": 0.4, \"bookshelf\": 0.2, \"cabinet\": 0.2, \"chaise_longue_sofa\": 0.3, \n",
    "        \"chinese_chair\": 0.2, \"coffee_table\": 0.3, \"console_table\": 0.3, \"corner_side_table\": 0.3, \n",
    "        \"desk\": 0.2, \"dining_chair\": 0.3, \"dining_table\": 0.4, \"l_shaped_sofa\": 0.3, \n",
    "        \"lazy_sofa\": 0.3, \"lounge_chair\": 0.3, \"loveseat_sofa\": 0.3, \"multi_seat_sofa\": 0.3, \n",
    "        \"pendant_lamp\": 0.6, \"round_end_table\": 0.3, \"shelf\": 0.2, \"stool\": 0.2, \"tv_stand\": 0.3, \n",
    "        \"wardrobe\": 0.2, \"wine_cabinet\": 0.2\n",
    "    },\n",
    "    \"chaise_longue_sofa\": {\n",
    "        \"armchair\": 0.5, \"bookshelf\": 0.3, \"cabinet\": 0.3, \"ceiling_lamp\": 0.3, \n",
    "        \"chinese_chair\": 0.4, \"coffee_table\": 0.6, \"console_table\": 0.4, \"corner_side_table\": 0.5, \n",
    "        \"desk\": 0.3, \"dining_chair\": 0.2, \"dining_table\": 0.2, \"l_shaped_sofa\": 0.7, \n",
    "        \"lazy_sofa\": 0.7, \"lounge_chair\": 0.6, \"loveseat_sofa\": 0.8, \"multi_seat_sofa\": 0.7, \n",
    "        \"pendant_lamp\": 0.5, \"round_end_table\": 0.6, \"shelf\": 0.3, \"stool\": 0.3, \"tv_stand\": 0.4, \n",
    "        \"wardrobe\": 0.3, \"wine_cabinet\": 0.3\n",
    "    },\n",
    "    \"chinese_chair\": {\n",
    "        \"armchair\": 0.3, \"bookshelf\": 0.3, \"cabinet\": 0.2, \"ceiling_lamp\": 0.2, \n",
    "        \"chaise_longue_sofa\": 0.4, \"coffee_table\": 0.4, \"console_table\": 0.3, \"corner_side_table\": 0.4, \n",
    "        \"desk\": 0.3, \"dining_chair\": 0.5, \"dining_table\": 0.5, \"l_shaped_sofa\": 0.3, \n",
    "        \"lazy_sofa\": 0.3, \"lounge_chair\": 0.3, \"loveseat_sofa\": 0.3, \"multi_seat_sofa\": 0.3, \n",
    "        \"pendant_lamp\": 0.3, \"round_end_table\": 0.3, \"shelf\": 0.2, \"stool\": 0.4, \"tv_stand\": 0.3, \n",
    "        \"wardrobe\": 0.2, \"wine_cabinet\": 0.1\n",
    "    },\n",
    "    \"coffee_table\": {\n",
    "        \"armchair\": 0.6, \"bookshelf\": 0.3, \"cabinet\": 0.2, \"ceiling_lamp\": 0.3, \n",
    "        \"chaise_longue_sofa\": 0.6, \"chinese_chair\": 0.4, \"console_table\": 0.5, \"corner_side_table\": 0.5, \n",
    "        \"desk\": 0.2, \"dining_chair\": 0.3, \"dining_table\": 0.3, \"l_shaped_sofa\": 0.7, \n",
    "        \"lazy_sofa\": 0.7, \"lounge_chair\": 0.7, \"loveseat_sofa\": 0.7, \"multi_seat_sofa\": 0.7, \n",
    "        \"pendant_lamp\": 0.6, \"round_end_table\": 0.5, \"shelf\": 0.3, \"stool\": 0.5, \"tv_stand\": 0.5, \n",
    "        \"wardrobe\": 0.3, \"wine_cabinet\": 0.3\n",
    "    },\n",
    "    \"console_table\": {\n",
    "        \"armchair\": 0.4, \"bookshelf\": 0.3, \"cabinet\": 0.3, \"ceiling_lamp\": 0.4, \n",
    "        \"chaise_longue_sofa\": 0.4, \"chinese_chair\": 0.3, \"coffee_table\": 0.5, \n",
    "        \"corner_side_table\": 0.4, \"desk\": 0.4, \"dining_chair\": 0.2, \"dining_table\": 0.2, \n",
    "        \"l_shaped_sofa\": 0.5, \"lazy_sofa\": 0.4, \"lounge_chair\": 0.4, \"loveseat_sofa\": 0.5, \n",
    "        \"multi_seat_sofa\": 0.4, \"pendant_lamp\": 0.7, \"round_end_table\": 0.5, \"shelf\": 0.3, \n",
    "        \"stool\": 0.3, \"tv_stand\": 0.6, \"wardrobe\": 0.2, \"wine_cabinet\": 0.3\n",
    "    },\n",
    "    \"corner_side_table\": {\n",
    "        \"armchair\": 0.5, \"bookshelf\": 0.2, \"cabinet\": 0.3, \"ceiling_lamp\": 0.3, \n",
    "        \"chaise_longue_sofa\": 0.4, \"chinese_chair\": 0.3, \"coffee_table\": 0.5, \n",
    "        \"console_table\": 0.4, \"desk\": 0.2, \"dining_chair\": 0.3, \"dining_table\": 0.2, \n",
    "        \"l_shaped_sofa\": 0.5, \"lazy_sofa\": 0.5, \"lounge_chair\": 0.6, \"loveseat_sofa\": 0.5, \n",
    "        \"multi_seat_sofa\": 0.4, \"pendant_lamp\": 0.7, \"round_end_table\": 0.6, \"shelf\": 0.3, \n",
    "        \"stool\": 0.4, \"tv_stand\": 0.3, \"wardrobe\": 0.2, \"wine_cabinet\": 0.2\n",
    "    },\n",
    "    \"dining_table\": {\n",
    "        \"armchair\": 0.2, \"bookshelf\": 0.1, \"cabinet\": 0.1, \"ceiling_lamp\": 0.4, \n",
    "        \"chaise_longue_sofa\": 0.2, \"chinese_chair\": 0.4, \"coffee_table\": 0.2, \n",
    "        \"console_table\": 0.2, \"corner_side_table\": 0.2, \"desk\": 0.3, \n",
    "        \"dining_chair\": 0.9, \"l_shaped_sofa\": 0.1, \"lazy_sofa\": 0.1, \n",
    "        \"lounge_chair\": 0.2, \"loveseat_sofa\": 0.1, \"multi_seat_sofa\": 0.1, \n",
    "        \"pendant_lamp\": 0.4, \"round_end_table\": 0.3, \"shelf\": 0.2, \"stool\": 0.3, \n",
    "        \"tv_stand\": 0.1, \"wardrobe\": 0.1, \"wine_cabinet\": 0.5\n",
    "    },\n",
    "    \"dining_chair\": {\n",
    "        \"armchair\": 0.2, \"bookshelf\": 0.1, \"cabinet\": 0.1, \"ceiling_lamp\": 0.3, \n",
    "        \"chaise_longue_sofa\": 0.2, \"chinese_chair\": 0.5, \"coffee_table\": 0.2, \n",
    "        \"console_table\": 0.2, \"corner_side_table\": 0.3, \"desk\": 0.4, \n",
    "        \"dining_table\": 0.9, \"l_shaped_sofa\": 0.2, \"lazy_sofa\": 0.2, \n",
    "        \"lounge_chair\": 0.3, \"loveseat_sofa\": 0.2, \"multi_seat_sofa\": 0.2, \n",
    "        \"pendant_lamp\": 0.3, \"round_end_table\": 0.3, \"shelf\": 0.2, \"stool\": 0.3, \n",
    "        \"tv_stand\": 0.1, \"wardrobe\": 0.1, \"wine_cabinet\": 0.4\n",
    "    },\n",
    "    \"l_shaped_sofa\": {\n",
    "    \"armchair\": 0.5, \"bookshelf\": 0.3, \"cabinet\": 0.2, \"ceiling_lamp\": 0.3,\n",
    "    \"chaise_longue_sofa\": 0.6, \"chinese_chair\": 0.3, \"coffee_table\": 0.7,\n",
    "    \"console_table\": 0.5, \"corner_side_table\": 0.5, \"desk\": 0.2,\n",
    "    \"dining_chair\": 0.1, \"dining_table\": 0.1, \"lazy_sofa\": 0.8,\n",
    "    \"lounge_chair\": 0.6, \"loveseat_sofa\": 0.7, \"multi_seat_sofa\": 0.8,\n",
    "    \"pendant_lamp\": 0.6, \"round_end_table\": 0.5, \"shelf\": 0.3, \"stool\": 0.4,\n",
    "    \"tv_stand\": 0.6, \"wardrobe\": 0.2, \"wine_cabinet\": 0.2\n",
    "    },\n",
    "    \"lazy_sofa\": {\n",
    "        \"armchair\": 0.5, \"bookshelf\": 0.3, \"cabinet\": 0.2, \"ceiling_lamp\": 0.3,\n",
    "        \"chaise_longue_sofa\": 0.7, \"chinese_chair\": 0.3, \"coffee_table\": 0.8,\n",
    "        \"console_table\": 0.5, \"corner_side_table\": 0.5, \"desk\": 0.2,\n",
    "        \"dining_chair\": 0.1, \"dining_table\": 0.1, \"l_shaped_sofa\": 0.8,\n",
    "        \"lounge_chair\": 0.7, \"loveseat_sofa\": 0.8, \"multi_seat_sofa\": 0.8,\n",
    "        \"pendant_lamp\": 0.6, \"round_end_table\": 0.6, \"shelf\": 0.3, \"stool\": 0.4,\n",
    "        \"tv_stand\": 0.6, \"wardrobe\": 0.2, \"wine_cabinet\": 0.2\n",
    "    },\n",
    "    \"lounge_chair\": {\n",
    "        \"armchair\": 0.7, \"bookshelf\": 0.3, \"cabinet\": 0.3, \"ceiling_lamp\": 0.3,\n",
    "        \"chaise_longue_sofa\": 0.5, \"chinese_chair\": 0.4, \"coffee_table\": 0.7,\n",
    "        \"console_table\": 0.4, \"corner_side_table\": 0.6, \"desk\": 0.3,\n",
    "        \"dining_chair\": 0.2, \"dining_table\": 0.2, \"l_shaped_sofa\": 0.6,\n",
    "        \"lazy_sofa\": 0.7, \"loveseat_sofa\": 0.7, \"multi_seat_sofa\": 0.6,\n",
    "        \"pendant_lamp\": 0.6, \"round_end_table\": 0.6, \"shelf\": 0.3, \"stool\": 0.4,\n",
    "        \"tv_stand\": 0.5, \"wardrobe\": 0.2, \"wine_cabinet\": 0.2\n",
    "    },\n",
    "    \"loveseat_sofa\": {\n",
    "        \"armchair\": 0.6, \"bookshelf\": 0.2, \"cabinet\": 0.2, \"ceiling_lamp\": 0.3,\n",
    "        \"chaise_longue_sofa\": 0.8, \"chinese_chair\": 0.3, \"coffee_table\": 0.8,\n",
    "        \"console_table\": 0.5, \"corner_side_table\": 0.5, \"desk\": 0.2,\n",
    "        \"dining_chair\": 0.2, \"dining_table\": 0.1, \"l_shaped_sofa\": 0.7,\n",
    "        \"lazy_sofa\": 0.8, \"lounge_chair\": 0.7, \"multi_seat_sofa\": 0.8,\n",
    "        \"pendant_lamp\": 0.6, \"round_end_table\": 0.6, \"shelf\": 0.2, \"stool\": 0.3,\n",
    "        \"tv_stand\": 0.5, \"wardrobe\": 0.2, \"wine_cabinet\": 0.2\n",
    "    },\n",
    "    \"multi_seat_sofa\": {\n",
    "        \"armchair\": 0.4, \"bookshelf\": 0.3, \"cabinet\": 0.2, \"ceiling_lamp\": 0.3,\n",
    "        \"chaise_longue_sofa\": 0.7, \"chinese_chair\": 0.3, \"coffee_table\": 0.8,\n",
    "        \"console_table\": 0.4, \"corner_side_table\": 0.4, \"desk\": 0.2,\n",
    "        \"dining_chair\": 0.2, \"dining_table\": 0.1, \"l_shaped_sofa\": 0.8,\n",
    "        \"lazy_sofa\": 0.8, \"lounge_chair\": 0.6, \"loveseat_sofa\": 0.8,\n",
    "        \"pendant_lamp\": 0.6, \"round_end_table\": 0.5, \"shelf\": 0.3, \"stool\": 0.4,\n",
    "        \"tv_stand\": 0.5, \"wardrobe\": 0.2, \"wine_cabinet\": 0.2\n",
    "    },\n",
    "    \"pendant_lamp\": {\n",
    "        \"armchair\": 0.6, \"bookshelf\": 0.4, \"cabinet\": 0.4, \"ceiling_lamp\": 0.7,\n",
    "        \"chaise_longue_sofa\": 0.5, \"chinese_chair\": 0.3, \"coffee_table\": 0.5,\n",
    "        \"console_table\": 0.7, \"corner_side_table\": 0.6, \"desk\": 0.6,\n",
    "        \"dining_chair\": 0.4, \"dining_table\": 0.4, \"l_shaped_sofa\": 0.6,\n",
    "        \"lazy_sofa\": 0.6, \"lounge_chair\": 0.6, \"loveseat_sofa\": 0.6,\n",
    "        \"multi_seat_sofa\": 0.5, \"round_end_table\": 0.7, \"shelf\": 0.5, \"stool\": 0.5,\n",
    "        \"tv_stand\": 0.5, \"wardrobe\": 0.3, \"wine_cabinet\": 0.4\n",
    "    },\n",
    "    \"round_end_table\": {\n",
    "        \"armchair\": 0.6, \"bookshelf\": 0.3, \"cabinet\": 0.3, \"ceiling_lamp\": 0.4,\n",
    "        \"chaise_longue_sofa\": 0.5, \"chinese_chair\": 0.3, \"coffee_table\": 0.7,\n",
    "        \"console_table\": 0.5, \"corner_side_table\": 0.6, \"desk\": 0.3,\n",
    "        \"dining_chair\": 0.3, \"dining_table\": 0.3, \"l_shaped_sofa\": 0.5,\n",
    "        \"lazy_sofa\": 0.6, \"lounge_chair\": 0.6, \"loveseat_sofa\": 0.5,\n",
    "        \"multi_seat_sofa\": 0.5, \"pendant_lamp\": 0.7, \"shelf\": 0.4, \"stool\": 0.5,\n",
    "        \"tv_stand\": 0.4, \"wardrobe\": 0.2, \"wine_cabinet\": 0.3\n",
    "    },\n",
    "    \"shelf\": {\n",
    "        \"armchair\": 0.3, \"bookshelf\": 0.8, \"cabinet\": 0.6, \"ceiling_lamp\": 0.3,\n",
    "        \"chaise_longue_sofa\": 0.2, \"chinese_chair\": 0.2, \"coffee_table\": 0.4,\n",
    "        \"console_table\": 0.4, \"corner_side_table\": 0.3, \"desk\": 0.5,\n",
    "        \"dining_chair\": 0.3, \"dining_table\": 0.3, \"l_shaped_sofa\": 0.2,\n",
    "        \"lazy_sofa\": 0.2, \"lounge_chair\": 0.3, \"loveseat_sofa\": 0.2,\n",
    "        \"multi_seat_sofa\": 0.2, \"pendant_lamp\": 0.5, \"round_end_table\": 0.4, \"stool\": 0.3,\n",
    "        \"tv_stand\": 0.4, \"wardrobe\": 0.5, \"wine_cabinet\": 0.6\n",
    "    },\n",
    "    \"stool\": {\n",
    "        \"armchair\": 0.4, \"bookshelf\": 0.2, \"cabinet\": 0.2, \"ceiling_lamp\": 0.3,\n",
    "        \"chaise_longue_sofa\": 0.3, \"chinese_chair\": 0.4, \"coffee_table\": 0.4,\n",
    "        \"console_table\": 0.3, \"corner_side_table\": 0.3, \"desk\": 0.4,\n",
    "        \"dining_chair\": 0.4, \"dining_table\": 0.3, \"l_shaped_sofa\": 0.4,\n",
    "        \"lazy_sofa\": 0.3, \"lounge_chair\": 0.4, \"loveseat_sofa\": 0.3,\n",
    "        \"multi_seat_sofa\": 0.3, \"pendant_lamp\": 0.5, \"round_end_table\": 0.5, \"shelf\": 0.3,\n",
    "        \"tv_stand\": 0.2, \"wardrobe\": 0.2, \"wine_cabinet\": 0.2\n",
    "    },\n",
    "    \"tv_stand\": {\n",
    "        \"armchair\": 0.5, \"bookshelf\": 0.3, \"cabinet\": 0.4, \"ceiling_lamp\": 0.3,\n",
    "        \"chaise_longue_sofa\": 0.5, \"chinese_chair\": 0.3, \"coffee_table\": 0.6,\n",
    "        \"console_table\": 0.6, \"corner_side_table\": 0.5, \"desk\": 0.2,\n",
    "        \"dining_chair\": 0.2, \"dining_table\": 0.1, \"l_shaped_sofa\": 0.6,\n",
    "        \"lazy_sofa\": 0.5, \"lounge_chair\": 0.5, \"loveseat_sofa\": 0.5,\n",
    "        \"multi_seat_sofa\": 0.5, \"pendant_lamp\": 0.5, \"round_end_table\": 0.4, \"shelf\": 0.4,\n",
    "        \"stool\": 0.3, \"wardrobe\": 0.2, \"wine_cabinet\": 0.3\n",
    "    },\n",
    "    \"wardrobe\": {\n",
    "        \"armchair\": 0.2, \"bookshelf\": 0.4, \"cabinet\": 0.5, \"ceiling_lamp\": 0.2,\n",
    "        \"chaise_longue_sofa\": 0.2, \"chinese_chair\": 0.2, \"coffee_table\": 0.2,\n",
    "        \"console_table\": 0.2, \"corner_side_table\": 0.2, \"desk\": 0.4,\n",
    "        \"dining_chair\": 0.1, \"dining_table\": 0.1, \"l_shaped_sofa\": 0.2,\n",
    "        \"lazy_sofa\": 0.2, \"lounge_chair\": 0.2, \"loveseat_sofa\": 0.2,\n",
    "        \"multi_seat_sofa\": 0.2, \"pendant_lamp\": 0.3, \"round_end_table\": 0.2, \"shelf\": 0.5,\n",
    "        \"stool\": 0.2, \"tv_stand\": 0.2, \"wine_cabinet\": 0.6\n",
    "    },\n",
    "    \"wine_cabinet\": {\n",
    "        \"armchair\": 0.2, \"bookshelf\": 0.4, \"cabinet\": 0.6, \"ceiling_lamp\": 0.2,\n",
    "        \"chaise_longue_sofa\": 0.2, \"chinese_chair\": 0.2, \"coffee_table\": 0.3,\n",
    "        \"console_table\": 0.3, \"corner_side_table\": 0.2, \"desk\": 0.2,\n",
    "        \"dining_chair\": 0.2, \"dining_table\": 0.5, \"l_shaped_sofa\": 0.2,\n",
    "        \"lazy_sofa\": 0.2, \"lounge_chair\": 0.3, \"loveseat_sofa\": 0.2,\n",
    "        \"multi_seat_sofa\": 0.2, \"pendant_lamp\": 0.4, \"round_end_table\": 0.3, \"shelf\": 0.6,\n",
    "        \"stool\": 0.2, \"tv_stand\": 0.3, \"wardrobe\": 0.6\n",
    "    }\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weighted sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted sums of relations for the selected objects:\n",
      "dining_table: 1.00\n",
      "dining_chair: 1.00\n",
      "wardrobe: 0.20\n"
     ]
    }
   ],
   "source": [
    "obj_list = [\"dining_table\", \"dining_chair\", \"wardrobe\", \"lamp\"]\n",
    "\n",
    "def calculate_weighted_sums_for_subset(weights_dict, object_subset):\n",
    "    weight_sums = {}\n",
    "    \n",
    "    for obj in object_subset:\n",
    "        if obj in weights_dict:\n",
    "            total_weight = 0.0\n",
    "            for related_obj, weight in weights_dict[obj].items():\n",
    "                if related_obj in object_subset:\n",
    "                    total_weight += weight\n",
    "            weight_sums[obj] = total_weight\n",
    "    \n",
    "    return weight_sums\n",
    "\n",
    "# Calculate and print the weighted sums for the specified subset of objects\n",
    "individual_weight_sums = calculate_weighted_sums_for_subset(weights, obj_list)\n",
    "print(\"Weighted sums of relations for the selected objects:\")\n",
    "for obj, weight_sum in individual_weight_sums.items():\n",
    "    print(f\"{obj}: {weight_sum:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cluster estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of objects in the scene: 8\n",
      "Optimal number of components based on BIC: 2\n",
      "\n",
      "Class labels for all objects in the scene:\n",
      "Object 0: dining_table (Class Index: 11)\n",
      "Object 1: dining_chair (Class Index: 10)\n",
      "Object 2: dining_chair (Class Index: 10)\n",
      "Object 3: dining_chair (Class Index: 10)\n",
      "Object 4: dining_chair (Class Index: 10)\n",
      "Object 5: console_table (Class Index: 7)\n",
      "Object 6: pendant_lamp (Class Index: 17)\n",
      "Object 7: wine_cabinet (Class Index: 23)\n",
      "\n",
      "Clustering results using GMM:\n",
      "\n",
      "Cluster 0 contains the following objects:\n",
      "  Object 0: dining_table\n",
      "  Object 1: dining_chair\n",
      "  Object 2: dining_chair\n",
      "  Object 3: dining_chair\n",
      "  Object 4: dining_chair\n",
      "  Object 7: wine_cabinet\n",
      "\n",
      "Cluster 1 contains the following objects:\n",
      "  Object 5: console_table\n",
      "  Object 6: pendant_lamp\n",
      "\n",
      "Detecting outliers using Elliptic Envelope...\n",
      "\n",
      "Outliers detected at indices: [6]\n",
      "Elliptic Envelope decision scores: [ 1.63472912e+10  1.63472912e+10  1.63472912e+10  1.63472912e+10\n",
      "  1.63472912e+10  1.63472912e+10 -3.81436817e+10  1.63472912e+10]\n",
      "\n",
      "Largest objects in each cluster by size:\n",
      "Cluster 0: Largest Object Index 7 with Area 1.501499891281128 and Label 'wine_cabinet'\n",
      "Cluster 1: Largest Object Index 5 with Area 0.36832237243652344 and Label 'console_table'\n",
      "\n",
      "Cluster 0 Tree Structure:\n",
      "Root: Object 7 (wine_cabinet)\n",
      "  â””â”€ Child Object 0 (dining_table)\n",
      "  â””â”€ Child Object 1 (dining_chair)\n",
      "  â””â”€ Child Object 2 (dining_chair)\n",
      "  â””â”€ Child Object 3 (dining_chair)\n",
      "  â””â”€ Child Object 4 (dining_chair)\n",
      "\n",
      "Cluster 1 Tree Structure:\n",
      "Root: Object 5 (console_table)\n",
      "  â””â”€ Child Object 6 (pendant_lamp)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gaurav/anaconda3/envs/grg/lib/python3.10/site-packages/sklearn/mixture/_base.py:134: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  .fit(X)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "\n",
    "scene_synthesis_path = 'path_to_scene_synthesis'\n",
    "\n",
    "sys.path.append(os.path.dirname(scene_synthesis_path))\n",
    "sys.path.append(scene_synthesis_path)\n",
    "\n",
    "from scene_synthesis.datasets.threed_front import ThreedFront\n",
    "from scene_synthesis.datasets.threed_future_dataset import ThreedFutureDataset\n",
    "from scene_synthesis.utils import get_textured_objects, get_floor_plan\n",
    "# from utils import export_scene, floor_plan_from_scene, render_scene_from_bbox_params, render_to_folder, render\n",
    "\n",
    "class_labels_dining = [\n",
    "    \"armchair\", \"bookshelf\", \"cabinet\", \"ceiling_lamp\", \n",
    "    \"chaise_longue_sofa\", \"chinese_chair\", \"coffee_table\", \n",
    "    \"console_table\", \"corner_side_table\", \"desk\", \"dining_chair\", \n",
    "    \"dining_table\", \"l_shaped_sofa\", \"lazy_sofa\", \"lounge_chair\", \n",
    "    \"loveseat_sofa\", \"multi_seat_sofa\", \"pendant_lamp\", \n",
    "    \"round_end_table\", \"shelf\", \"stool\", \"tv_stand\", \n",
    "    \"wardrobe\", \"wine_cabinet\", \"start\", \"end\"]\n",
    "\n",
    "class_labels_bedroom = [\n",
    "    \"armchair\", \"bookshelf\", \"cabinet\", \"ceiling_lamp\", \"chair\", \n",
    "    \"children_cabinet\", \"coffee_table\", \"desk\", \"double_bed\", \n",
    "    \"dressing_chair\", \"dressing_table\", \"kids_bed\", \"nightstand\", \n",
    "    \"pendant_lamp\", \"shelf\", \"single_bed\", \"sofa\", \"stool\", \"table\", \n",
    "    \"tv_stand\", \"wardrobe\", \"start\", \"end\"]\n",
    "\n",
    "def fetch_scene_id(all_scene_paths, scene_id):\n",
    "    for scene in all_scene_paths:\n",
    "        if scene_id in scene:\n",
    "            return scene\n",
    "    return None\n",
    "\n",
    "def load_attributes_from_npz(scene_path, attrib_list=[]):\n",
    "    scene = np.load(os.path.join(scene_path, 'boxes.npz'))\n",
    "    if not attrib_list:\n",
    "        attrib_list = list(scene.keys())\n",
    "    vals = {}\n",
    "    for attrib in attrib_list:\n",
    "        vals[attrib] = scene[attrib]\n",
    "    return vals\n",
    "\n",
    "def calculate_giou(bbox1, bbox2):\n",
    "    def area(bbox):\n",
    "        return max(0, bbox[2] - bbox[0]) * max(0, bbox[3] - bbox[1])\n",
    "\n",
    "    xA = max(bbox1[0], bbox2[0])\n",
    "    yA = max(bbox1[1], bbox2[1])\n",
    "    xB = min(bbox1[2], bbox2[2])\n",
    "    yB = min(bbox1[3], bbox2[3])\n",
    "    intersection = area([xA, yA, xB, yB])\n",
    "    \n",
    "    union = area(bbox1) + area(bbox2) - intersection\n",
    "    \n",
    "    iou = intersection / union if union > 0 else 0\n",
    "    \n",
    "    xC = min(bbox1[0], bbox2[0])\n",
    "    yC = min(bbox1[1], bbox2[1])\n",
    "    xD = max(bbox1[2], bbox2[2])\n",
    "    yD = max(bbox1[3], bbox2[3])\n",
    "    enclosing_area = area([xC, yC, xD, yD])\n",
    "    \n",
    "    giou = iou - ((enclosing_area - union) / enclosing_area)\n",
    "    return giou\n",
    "\n",
    "def dist_matrix(translations, sizes, lambda_value=0.02):\n",
    "    num_boxes = len(translations)\n",
    "    distance_matrix = np.zeros((num_boxes, num_boxes))\n",
    "    \n",
    "    for i in range(num_boxes):\n",
    "        for j in range(num_boxes):\n",
    "            if i != j:\n",
    "                bbox_i = [\n",
    "                    translations[i][0] - sizes[i][0] / 2,\n",
    "                    translations[i][1] - sizes[i][1] / 2,\n",
    "                    translations[i][0] + sizes[i][0] / 2,\n",
    "                    translations[i][1] + sizes[i][1] / 2\n",
    "                ]\n",
    "                \n",
    "                bbox_j = [\n",
    "                    translations[j][0] - sizes[j][0] / 2,\n",
    "                    translations[j][1] - sizes[j][1] / 2,\n",
    "                    translations[j][0] + sizes[j][0] / 2,\n",
    "                    translations[j][1] + sizes[j][1] / 2\n",
    "                ]\n",
    "                \n",
    "                center_i = (translations[i][0], translations[i][1])\n",
    "                center_j = (translations[j][0], translations[j][1])\n",
    "                euclidean_distance = np.linalg.norm(np.array(center_i) - np.array(center_j))\n",
    "                \n",
    "                giou = calculate_giou(bbox_i, bbox_j)\n",
    "                \n",
    "                distance_matrix[i, j] = euclidean_distance + lambda_value * (1 - giou)\n",
    "\n",
    "    return distance_matrix\n",
    "\n",
    "def extract_features_from_bboxes(attrib):\n",
    "    class_labels = attrib[\"class_labels\"].argmax(-1)\n",
    "    translations = attrib[\"translations\"]\n",
    "    sizes = attrib[\"sizes\"]\n",
    "    angles = np.squeeze(attrib[\"angles\"])\n",
    "    \n",
    "    features_translations = translations[:,:2]\n",
    "    features_sizes = sizes[:,:2]\n",
    "    return features_translations, features_sizes\n",
    "\n",
    "def cluster_scene_objects_gmm(distance_matrix, max_components=10):\n",
    "    bic_scores = []\n",
    "    n_components_range = range(1, min(max_components, len(distance_matrix)) + 1)\n",
    "    \n",
    "    for n_components in n_components_range:\n",
    "        gmm = GaussianMixture(n_components=n_components, covariance_type='full')\n",
    "        gmm.fit(distance_matrix)\n",
    "        bic_scores.append(gmm.bic(distance_matrix))\n",
    "    \n",
    "    # Select the number of components with the lowest BIC score\n",
    "    optimal_n_components = n_components_range[np.argmin(bic_scores)]\n",
    "    print(f\"Optimal number of components based on BIC: {optimal_n_components}\")\n",
    "    \n",
    "    gmm_optimal = GaussianMixture(n_components=optimal_n_components, covariance_type='full')\n",
    "    cluster_labels = gmm_optimal.fit_predict(distance_matrix)\n",
    "    return cluster_labels\n",
    "\n",
    "def detect_outliers_elliptic_envelope(features, contamination=0.1):\n",
    "    scaler = StandardScaler()\n",
    "    features_std = scaler.fit_transform(features)\n",
    "\n",
    "    ee = EllipticEnvelope(contamination=contamination)\n",
    "    ee.fit(features_std)\n",
    "    \n",
    "    outlier_pred = ee.predict(features_std)\n",
    "    \n",
    "    outliers = np.where(outlier_pred == -1)[0]\n",
    "    \n",
    "    decision_scores = ee.decision_function(features_std)\n",
    "    \n",
    "    return outliers, decision_scores\n",
    "\n",
    "def find_largest_object_in_clusters(features_sizes, cluster_labels, class_labels):\n",
    "    cluster_largest_objects = {}\n",
    "    areas = np.prod(features_sizes, axis=1)\n",
    "    unique_clusters = np.unique(cluster_labels)\n",
    "    for cluster in unique_clusters:\n",
    "        cluster_indices = np.where(cluster_labels == cluster)[0]\n",
    "        cluster_areas = areas[cluster_indices]\n",
    "        \n",
    "        largest_index_within_cluster = cluster_indices[np.argmax(cluster_areas)]\n",
    "        largest_class_label = class_labels[largest_index_within_cluster]\n",
    "        cluster_largest_objects[cluster] = {\n",
    "            \"index\": largest_index_within_cluster,\n",
    "            \"area\": cluster_areas.max(),\n",
    "            \"label\": largest_class_label\n",
    "        }\n",
    "    \n",
    "    return cluster_largest_objects\n",
    "\n",
    "def build_tree(cluster_largest_objects, cluster_labels, features_sizes, class_labels, label_names):\n",
    "    cluster_trees = {}\n",
    "    for cluster, info in cluster_largest_objects.items():\n",
    "        root_label = label_names[info['label']]\n",
    "        root_index = info['index']\n",
    "        \n",
    "        # Initialize the root node of the tree with the largest object\n",
    "        tree = {root_index: {\"label\": root_label, \"children\": []}}\n",
    "        cluster_indices = np.where(cluster_labels == cluster)[0]\n",
    "        \n",
    "        for idx in cluster_indices:\n",
    "            if idx == root_index:\n",
    "                continue  # skip the root node\n",
    "            \n",
    "            child_label = label_names[class_labels[idx]]\n",
    "            \n",
    "            # Attach all items directly under the root node\n",
    "            tree[root_index][\"children\"].append({\"index\": idx, \"label\": child_label, \"children\": []})\n",
    "        \n",
    "        cluster_trees[cluster] = tree\n",
    "\n",
    "    return cluster_trees\n",
    "\n",
    "def main(scene_id):\n",
    "    processed_path = '/home/gaurav/Desktop/furniture_code/3d_front_preprocessed/'\n",
    "    if not os.path.exists(processed_path):\n",
    "        print(f\"Processed path {processed_path} does not exist.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    all_paths = [os.path.join(processed_path, dir_) for dir_ in os.listdir(processed_path)]\n",
    "    \n",
    "    all_scenes = []\n",
    "    for path_ in all_paths:\n",
    "        if os.path.isdir(path_):\n",
    "            all_scenes.extend([os.path.join(path_, x) for x in os.listdir(path_) if os.path.isdir(os.path.join(path_, x))])\n",
    "    \n",
    "    scene_path = fetch_scene_id(all_scenes, scene_id)\n",
    "    if scene_path is None:\n",
    "        print(f\"Scene ID {scene_id} not found.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    required_attribs = ['class_labels', 'translations', 'sizes', 'angles']\n",
    "    attribs = load_attributes_from_npz(scene_path, required_attribs)\n",
    "    \n",
    "    for attrib in required_attribs:\n",
    "        if attrib not in attribs:\n",
    "            print(f\"Attribute '{attrib}' not found in the NPZ file.\")\n",
    "            sys.exit(1)\n",
    "    \n",
    "    features_translations, features_sizes = extract_features_from_bboxes(attribs)\n",
    "\n",
    "    total_objects = attribs['class_labels'].shape[0]\n",
    "    print(f\"Total number of objects in the scene: {total_objects}\")\n",
    "\n",
    "    distance_matrix = dist_matrix(features_translations, features_sizes)\n",
    "\n",
    "    # Determine optimal number of clusters using BIC\n",
    "    cluster_labels = cluster_scene_objects_gmm(distance_matrix, max_components=total_objects)\n",
    "\n",
    "    class_labels = attribs['class_labels'].argmax(-1)\n",
    "\n",
    "    print(\"\\nClass labels for all objects in the scene:\")\n",
    "    for i, label_idx in enumerate(class_labels):\n",
    "        print(f\"Object {i}: {class_labels_dining[label_idx]} (Class Index: {label_idx})\")\n",
    "\n",
    "    print(f\"\\nClustering results using GMM:\")\n",
    "    cluster_dict = {}\n",
    "\n",
    "    for idx, label in enumerate(cluster_labels):\n",
    "        if label not in cluster_dict:\n",
    "            cluster_dict[label] = []\n",
    "        cluster_dict[label].append(idx)\n",
    "\n",
    "    for label, indices in cluster_dict.items():\n",
    "        print(f\"\\nCluster {label} contains the following objects:\")\n",
    "        for idx in indices:\n",
    "            print(f\"  Object {idx}: {class_labels_dining[class_labels[idx]]}\")\n",
    "\n",
    "    print(\"\\nDetecting outliers using Elliptic Envelope...\")\n",
    "    features = np.hstack((features_translations, features_sizes))\n",
    "    outliers, decision_scores = detect_outliers_elliptic_envelope(features)\n",
    "    \n",
    "    print(f\"\\nOutliers detected at indices: {outliers}\")\n",
    "    print(f\"Elliptic Envelope decision scores: {decision_scores}\")\n",
    "\n",
    "    cluster_largest_objects = find_largest_object_in_clusters(features_sizes, cluster_labels, class_labels)\n",
    "\n",
    "    print(\"\\nLargest objects in each cluster by size:\")\n",
    "    for cluster, info in cluster_largest_objects.items():\n",
    "        label_name = class_labels_dining[info['label']]\n",
    "        print(f\"Cluster {cluster}: Largest Object Index {info['index']} with Area {info['area']} and Label '{label_name}'\")\n",
    "\n",
    "    cluster_trees = build_tree(cluster_largest_objects, cluster_labels, features_sizes, class_labels, class_labels_dining)\n",
    "\n",
    "    for cluster, tree in cluster_trees.items():\n",
    "        print(f\"\\nCluster {cluster} Tree Structure:\")\n",
    "        for node, data in tree.items():\n",
    "            print(f\"Root: Object {node} ({data['label']})\")\n",
    "            for child in data[\"children\"]:\n",
    "                print(f\"  â””â”€ Child Object {child['index']} ({child['label']})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(scene_id='DiningRoom-9450')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_semantic_spatial_score(idx, cluster_indices, class_labels, distance_matrix, weights, label_names):\n",
    "    score = 0.0\n",
    "    count = 0\n",
    "    label_i = label_names[class_labels[idx]]\n",
    "    for j in cluster_indices:\n",
    "        if j == idx:\n",
    "            continue\n",
    "        label_j = label_names[class_labels[j]]\n",
    "        weight = weights.get(label_i, {}).get(label_j, 0.0)\n",
    "        dist = distance_matrix[idx, j]\n",
    "        score += weight / (1.0 + dist)\n",
    "        count += 1\n",
    "    return score / count if count > 0 else 0.0\n",
    "\n",
    "def refine_clusters_by_weights(cluster_labels, distance_matrix, class_labels, weights, label_names, threshold=0.4):\n",
    "    refined_labels = cluster_labels.copy()\n",
    "    outliers = []\n",
    "    for cluster_id in np.unique(cluster_labels):\n",
    "        indices = np.where(cluster_labels == cluster_id)[0]\n",
    "        for idx in indices:\n",
    "            score = compute_semantic_spatial_score(idx, indices, class_labels, distance_matrix, weights, label_names)\n",
    "            if score < threshold:\n",
    "                outliers.append(idx)\n",
    "                refined_labels[idx] = -1\n",
    "    return refined_labels, outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of objects in the scene: 8\n",
      "Optimal number of components based on BIC: 4\n",
      "\n",
      "Semantic-Spatial Outliers (based on affinity score): [5, 6, 7]\n",
      "\n",
      "Class labels for all objects in the scene:\n",
      "Object 0: dining_table (Class Index: 11)\n",
      "Object 1: dining_chair (Class Index: 10)\n",
      "Object 2: dining_chair (Class Index: 10)\n",
      "Object 3: dining_chair (Class Index: 10)\n",
      "Object 4: dining_chair (Class Index: 10)\n",
      "Object 5: console_table (Class Index: 7)\n",
      "Object 6: pendant_lamp (Class Index: 17)\n",
      "Object 7: wine_cabinet (Class Index: 23)\n",
      "\n",
      "Clustering results after refinement:\n",
      "\n",
      "Cluster 0 contains the following objects:\n",
      "  Object 0: dining_table\n",
      "  Object 1: dining_chair\n",
      "  Object 2: dining_chair\n",
      "  Object 3: dining_chair\n",
      "  Object 4: dining_chair\n",
      "\n",
      "Cluster -1 contains the following objects:\n",
      "  Object 5: console_table\n",
      "  Object 6: pendant_lamp\n",
      "  Object 7: wine_cabinet\n",
      "\n",
      "Detecting outliers using Elliptic Envelope...\n",
      "\n",
      "Elliptic Envelope Outliers: [6]\n",
      "Decision Scores: [ 1.63472912e+10  1.63472912e+10  1.63472912e+10  1.63472912e+10\n",
      "  1.63472912e+10  1.63472912e+10 -3.81436817e+10  1.63472912e+10]\n",
      "\n",
      "Largest objects in each cluster by size:\n",
      "Cluster -1: Largest Object Index 7 with Area 1.501499891281128 and Label 'wine_cabinet'\n",
      "Cluster 0: Largest Object Index 0 with Area 0.30799993872642517 and Label 'dining_table'\n",
      "\n",
      "Cluster -1 Tree Structure:\n",
      "Root: Object 7 (wine_cabinet)\n",
      "  â””â”€ Child Object 5 (console_table)\n",
      "  â””â”€ Child Object 6 (pendant_lamp)\n",
      "\n",
      "Cluster 0 Tree Structure:\n",
      "Root: Object 0 (dining_table)\n",
      "  â””â”€ Child Object 1 (dining_chair)\n",
      "  â””â”€ Child Object 2 (dining_chair)\n",
      "  â””â”€ Child Object 3 (dining_chair)\n",
      "  â””â”€ Child Object 4 (dining_chair)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gaurav/anaconda3/envs/grg/lib/python3.10/site-packages/sklearn/mixture/_base.py:134: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  .fit(X)\n"
     ]
    }
   ],
   "source": [
    "def main(scene_id):\n",
    "    processed_path = '/home/gaurav/Desktop/furniture_code/3d_front_preprocessed/'\n",
    "    if not os.path.exists(processed_path):\n",
    "        print(f\"Processed path {processed_path} does not exist.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    all_paths = [os.path.join(processed_path, dir_) for dir_ in os.listdir(processed_path)]\n",
    "\n",
    "    all_scenes = []\n",
    "    for path_ in all_paths:\n",
    "        if os.path.isdir(path_):\n",
    "            all_scenes.extend([os.path.join(path_, x) for x in os.listdir(path_) if os.path.isdir(os.path.join(path_, x))])\n",
    "\n",
    "    scene_path = fetch_scene_id(all_scenes, scene_id)\n",
    "    if scene_path is None:\n",
    "        print(f\"Scene ID {scene_id} not found.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    required_attribs = ['class_labels', 'translations', 'sizes', 'angles']\n",
    "    attribs = load_attributes_from_npz(scene_path, required_attribs)\n",
    "\n",
    "    features_translations, features_sizes = extract_features_from_bboxes(attribs)\n",
    "    total_objects = attribs['class_labels'].shape[0]\n",
    "    print(f\"Total number of objects in the scene: {total_objects}\")\n",
    "\n",
    "    distance_matrix = dist_matrix(features_translations, features_sizes)\n",
    "    cluster_labels = cluster_scene_objects_gmm(distance_matrix, max_components=total_objects)\n",
    "\n",
    "    class_labels = attribs['class_labels'].argmax(-1)\n",
    "\n",
    "    # Refine clusters using semantic-spatial weights\n",
    "    refined_cluster_labels, semantic_outliers = refine_clusters_by_weights(\n",
    "        cluster_labels, distance_matrix, class_labels, weights, class_labels_dining, threshold=0.15\n",
    "    )\n",
    "\n",
    "    print(\"\\nSemantic-Spatial Outliers (based on affinity score):\", semantic_outliers)\n",
    "\n",
    "    print(\"\\nClass labels for all objects in the scene:\")\n",
    "    for i, label_idx in enumerate(class_labels):\n",
    "        print(f\"Object {i}: {class_labels_dining[label_idx]} (Class Index: {label_idx})\")\n",
    "\n",
    "    print(\"\\nClustering results after refinement:\")\n",
    "    cluster_dict = {}\n",
    "    for idx, label in enumerate(refined_cluster_labels):\n",
    "        if label not in cluster_dict:\n",
    "            cluster_dict[label] = []\n",
    "        cluster_dict[label].append(idx)\n",
    "\n",
    "    for label, indices in cluster_dict.items():\n",
    "        print(f\"\\nCluster {label} contains the following objects:\")\n",
    "        for idx in indices:\n",
    "            print(f\"  Object {idx}: {class_labels_dining[class_labels[idx]]}\")\n",
    "\n",
    "    print(\"\\nDetecting outliers using Elliptic Envelope...\")\n",
    "    features = np.hstack((features_translations, features_sizes))\n",
    "    outliers, decision_scores = detect_outliers_elliptic_envelope(features)\n",
    "    print(f\"\\nElliptic Envelope Outliers: {outliers}\")\n",
    "    print(f\"Decision Scores: {decision_scores}\")\n",
    "\n",
    "    cluster_largest_objects = find_largest_object_in_clusters(features_sizes, refined_cluster_labels, class_labels)\n",
    "\n",
    "    print(\"\\nLargest objects in each cluster by size:\")\n",
    "    for cluster, info in cluster_largest_objects.items():\n",
    "        label_name = class_labels_dining[info['label']]\n",
    "        print(f\"Cluster {cluster}: Largest Object Index {info['index']} with Area {info['area']} and Label '{label_name}'\")\n",
    "\n",
    "    cluster_trees = build_tree(cluster_largest_objects, refined_cluster_labels, features_sizes, class_labels, class_labels_dining)\n",
    "\n",
    "    for cluster, tree in cluster_trees.items():\n",
    "        print(f\"\\nCluster {cluster} Tree Structure:\")\n",
    "        for node, data in tree.items():\n",
    "            print(f\"Root: Object {node} ({data['label']})\")\n",
    "            for child in data[\"children\"]:\n",
    "                print(f\"  â””â”€ Child Object {child['index']} ({child['label']})\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(scene_id='DiningRoom-9450')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of objects in the scene: 8\n",
      "Optimal number of components based on BIC: 2\n",
      "\n",
      "Semantic-Spatial Outliers (based on affinity score): []\n",
      "\n",
      "Class labels for all objects in the scene:\n",
      "Object 0: dining_table (Class Index: 11)\n",
      "Object 1: dining_chair (Class Index: 10)\n",
      "Object 2: dining_chair (Class Index: 10)\n",
      "Object 3: dining_chair (Class Index: 10)\n",
      "Object 4: dining_chair (Class Index: 10)\n",
      "Object 5: console_table (Class Index: 7)\n",
      "Object 6: pendant_lamp (Class Index: 17)\n",
      "Object 7: wine_cabinet (Class Index: 23)\n",
      "\n",
      "Clustering results after refinement:\n",
      "\n",
      "Cluster 0 contains the following objects:\n",
      "  Object 0: dining_table\n",
      "  Object 1: dining_chair\n",
      "  Object 2: dining_chair\n",
      "  Object 3: dining_chair\n",
      "  Object 4: dining_chair\n",
      "  Object 7: wine_cabinet\n",
      "\n",
      "Cluster 1 contains the following objects:\n",
      "  Object 5: console_table\n",
      "  Object 6: pendant_lamp\n",
      "\n",
      "Detecting outliers using Elliptic Envelope...\n",
      "\n",
      "Elliptic Envelope Outliers: [6]\n",
      "Decision Scores: [ 1.63472912e+10  1.63472912e+10  1.63472912e+10  1.63472912e+10\n",
      "  1.63472912e+10  1.63472912e+10 -3.81436817e+10  1.63472912e+10]\n",
      "\n",
      "Largest objects in each cluster by size:\n",
      "Cluster 0: Largest Object Index 7 with Area 1.501499891281128 and Label 'wine_cabinet'\n",
      "Cluster 1: Largest Object Index 5 with Area 0.36832237243652344 and Label 'console_table'\n",
      "\n",
      "Cluster 0 Tree Structure:\n",
      "Root: Object 7 (wine_cabinet)\n",
      "  â””â”€ Child Object 0 (dining_table)\n",
      "  â””â”€ Child Object 1 (dining_chair)\n",
      "  â””â”€ Child Object 2 (dining_chair)\n",
      "  â””â”€ Child Object 3 (dining_chair)\n",
      "  â””â”€ Child Object 4 (dining_chair)\n",
      "\n",
      "Cluster 1 Tree Structure:\n",
      "Root: Object 5 (console_table)\n",
      "  â””â”€ Child Object 6 (pendant_lamp)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gaurav/anaconda3/envs/grg/lib/python3.10/site-packages/sklearn/mixture/_base.py:134: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  .fit(X)\n"
     ]
    }
   ],
   "source": [
    "def refine_clusters_by_weights(cluster_labels, distance_matrix, class_labels, weights, label_names, threshold=0.4):\n",
    "    refined_labels = cluster_labels.copy()\n",
    "    outliers = []\n",
    "\n",
    "    next_cluster_id = max(refined_labels) + 1\n",
    "\n",
    "    for cluster_id in np.unique(cluster_labels):\n",
    "        indices = np.where(cluster_labels == cluster_id)[0]\n",
    "        for idx in indices:\n",
    "            score = compute_semantic_spatial_score(idx, indices, class_labels, distance_matrix, weights, label_names)\n",
    "            if score < threshold:\n",
    "                outliers.append(idx)\n",
    "                refined_labels[idx] = next_cluster_id\n",
    "                next_cluster_id += 1\n",
    "\n",
    "    return refined_labels, outliers\n",
    "\n",
    "\n",
    "def main(scene_id):\n",
    "    processed_path = '/home/gaurav/Desktop/furniture_code/3d_front_preprocessed/'\n",
    "    if not os.path.exists(processed_path):\n",
    "        print(f\"Processed path {processed_path} does not exist.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    all_paths = [os.path.join(processed_path, dir_) for dir_ in os.listdir(processed_path)]\n",
    "\n",
    "    all_scenes = []\n",
    "    for path_ in all_paths:\n",
    "        if os.path.isdir(path_):\n",
    "            all_scenes.extend([os.path.join(path_, x) for x in os.listdir(path_) if os.path.isdir(os.path.join(path_, x))])\n",
    "\n",
    "    scene_path = fetch_scene_id(all_scenes, scene_id)\n",
    "    if scene_path is None:\n",
    "        print(f\"Scene ID {scene_id} not found.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    required_attribs = ['class_labels', 'translations', 'sizes', 'angles']\n",
    "    attribs = load_attributes_from_npz(scene_path, required_attribs)\n",
    "\n",
    "    features_translations, features_sizes = extract_features_from_bboxes(attribs)\n",
    "    total_objects = attribs['class_labels'].shape[0]\n",
    "    print(f\"Total number of objects in the scene: {total_objects}\")\n",
    "\n",
    "    distance_matrix = dist_matrix(features_translations, features_sizes)\n",
    "    cluster_labels = cluster_scene_objects_gmm(distance_matrix, max_components=total_objects)\n",
    "\n",
    "    class_labels = attribs['class_labels'].argmax(-1)\n",
    "\n",
    "    # Refine clusters using semantic-spatial weights\n",
    "    refined_cluster_labels, semantic_outliers = refine_clusters_by_weights(\n",
    "        cluster_labels, distance_matrix, class_labels, weights, class_labels_dining, threshold=0.1\n",
    "    )\n",
    "\n",
    "    print(\"\\nSemantic-Spatial Outliers (based on affinity score):\", semantic_outliers)\n",
    "\n",
    "    print(\"\\nClass labels for all objects in the scene:\")\n",
    "    for i, label_idx in enumerate(class_labels):\n",
    "        print(f\"Object {i}: {class_labels_dining[label_idx]} (Class Index: {label_idx})\")\n",
    "\n",
    "    print(\"\\nClustering results after refinement:\")\n",
    "    cluster_dict = {}\n",
    "    for idx, label in enumerate(refined_cluster_labels):\n",
    "        if label not in cluster_dict:\n",
    "            cluster_dict[label] = []\n",
    "        cluster_dict[label].append(idx)\n",
    "\n",
    "    for label, indices in cluster_dict.items():\n",
    "        print(f\"\\nCluster {label} contains the following objects:\")\n",
    "        for idx in indices:\n",
    "            print(f\"  Object {idx}: {class_labels_dining[class_labels[idx]]}\")\n",
    "\n",
    "    print(\"\\nDetecting outliers using Elliptic Envelope...\")\n",
    "    features = np.hstack((features_translations, features_sizes))\n",
    "    outliers, decision_scores = detect_outliers_elliptic_envelope(features)\n",
    "    print(f\"\\nElliptic Envelope Outliers: {outliers}\")\n",
    "    print(f\"Decision Scores: {decision_scores}\")\n",
    "\n",
    "    cluster_largest_objects = find_largest_object_in_clusters(features_sizes, refined_cluster_labels, class_labels)\n",
    "\n",
    "    print(\"\\nLargest objects in each cluster by size:\")\n",
    "    for cluster, info in cluster_largest_objects.items():\n",
    "        label_name = class_labels_dining[info['label']]\n",
    "        print(f\"Cluster {cluster}: Largest Object Index {info['index']} with Area {info['area']} and Label '{label_name}'\")\n",
    "\n",
    "    cluster_trees = build_tree(cluster_largest_objects, refined_cluster_labels, features_sizes, class_labels, class_labels_dining)\n",
    "\n",
    "    for cluster, tree in cluster_trees.items():\n",
    "        print(f\"\\nCluster {cluster} Tree Structure:\")\n",
    "        for node, data in tree.items():\n",
    "            print(f\"Root: Object {node} ({data['label']})\")\n",
    "            for child in data[\"children\"]:\n",
    "                print(f\"  â””â”€ Child Object {child['index']} ({child['label']})\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(scene_id='DiningRoom-9450')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_clusters_after_estimation(cluster_labels, features_sizes, class_labels, distance_matrix, weights, label_names, threshold=0.3):\n",
    "    refined_labels = cluster_labels.copy()\n",
    "    outliers = []\n",
    "    next_cluster_id = max(refined_labels) + 1\n",
    "\n",
    "    # Step 1: Get root (largest) object per cluster\n",
    "    cluster_roots = find_largest_object_in_clusters(features_sizes, cluster_labels, class_labels)\n",
    "\n",
    "    for cluster_id in np.unique(cluster_labels):\n",
    "        root_info = cluster_roots[cluster_id]\n",
    "        root_idx = root_info[\"index\"]\n",
    "        root_label = label_names[class_labels[root_idx]]\n",
    "\n",
    "        cluster_indices = np.where(refined_labels == cluster_id)[0]\n",
    "\n",
    "        for idx in cluster_indices:\n",
    "            if idx == root_idx:\n",
    "                continue\n",
    "\n",
    "            obj_label = label_names[class_labels[idx]]\n",
    "            weight = weights.get(root_label, {}).get(obj_label, 0.0)\n",
    "            distance = distance_matrix[root_idx, idx]\n",
    "            support = weight / (1.0 + distance)\n",
    "\n",
    "            print(f\"[Cluster {cluster_id}] Root: {root_label} ({root_idx}), Object: {obj_label} ({idx})\")\n",
    "            print(f\"    Weight: {weight}, Distance: {distance:.3f}, Support Score: {support:.3f}\")\n",
    "\n",
    "            if support < threshold:\n",
    "                refined_labels[idx] = next_cluster_id\n",
    "                outliers.append(idx)\n",
    "                print(f\"    âž¤ OUTLIER â†’ New Cluster {next_cluster_id}\")\n",
    "                next_cluster_id += 1\n",
    "            else:\n",
    "                print(f\"    âœ“ KEPT in Cluster {cluster_id}\")\n",
    "\n",
    "    return refined_labels, outliers\n",
    "\n",
    "def find_semantic_root_in_clusters(class_labels, cluster_labels, weights, label_names):\n",
    "    cluster_roots = {}\n",
    "\n",
    "    for cluster in np.unique(cluster_labels):\n",
    "        indices = np.where(cluster_labels == cluster)[0]\n",
    "        max_support = -1\n",
    "        root_idx = -1\n",
    "\n",
    "        for i in indices:\n",
    "            label_i = label_names[class_labels[i]]\n",
    "            support = 0\n",
    "            for j in indices:\n",
    "                if i == j:\n",
    "                    continue\n",
    "                label_j = label_names[class_labels[j]]\n",
    "                support += weights.get(label_i, {}).get(label_j, 0.0)\n",
    "\n",
    "            if support > max_support:\n",
    "                max_support = support\n",
    "                root_idx = i\n",
    "\n",
    "        cluster_roots[cluster] = {\n",
    "            \"index\": root_idx,\n",
    "            \"label\": class_labels[root_idx],\n",
    "            \"support\": max_support\n",
    "        }\n",
    "\n",
    "    return cluster_roots\n",
    "\n",
    "\n",
    "\n",
    "def main(scene_id):\n",
    "    processed_path = '/home/gaurav/Desktop/furniture_code/3d_front_preprocessed/'\n",
    "    if not os.path.exists(processed_path):\n",
    "        print(f\"Processed path {processed_path} does not exist.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    all_paths = [os.path.join(processed_path, dir_) for dir_ in os.listdir(processed_path)]\n",
    "\n",
    "    all_scenes = []\n",
    "    for path_ in all_paths:\n",
    "        if os.path.isdir(path_):\n",
    "            all_scenes.extend([os.path.join(path_, x) for x in os.listdir(path_) if os.path.isdir(os.path.join(path_, x))])\n",
    "\n",
    "    scene_path = fetch_scene_id(all_scenes, scene_id)\n",
    "    if scene_path is None:\n",
    "        print(f\"Scene ID {scene_id} not found.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    required_attribs = ['class_labels', 'translations', 'sizes', 'angles']\n",
    "    attribs = load_attributes_from_npz(scene_path, required_attribs)\n",
    "\n",
    "    features_translations, features_sizes = extract_features_from_bboxes(attribs)\n",
    "    total_objects = attribs['class_labels'].shape[0]\n",
    "    print(f\"Total number of objects in the scene: {total_objects}\")\n",
    "\n",
    "    distance_matrix = dist_matrix(features_translations, features_sizes)\n",
    "    cluster_labels = cluster_scene_objects_gmm(distance_matrix, max_components=total_objects)\n",
    "\n",
    "    class_labels = attribs['class_labels'].argmax(-1)\n",
    "\n",
    "    refined_cluster_labels, semantic_outliers = refine_clusters_after_estimation(\n",
    "        cluster_labels,\n",
    "        features_sizes,\n",
    "        class_labels,\n",
    "        distance_matrix,\n",
    "        weights,\n",
    "        class_labels_dining,\n",
    "        threshold=0.01\n",
    "    )\n",
    "\n",
    "\n",
    "    print(\"\\nSemantic-Spatial Outliers (based on affinity score):\", semantic_outliers)\n",
    "\n",
    "    print(\"\\nClass labels for all objects in the scene:\")\n",
    "    for i, label_idx in enumerate(class_labels):\n",
    "        print(f\"Object {i}: {class_labels_dining[label_idx]} (Class Index: {label_idx})\")\n",
    "\n",
    "    print(\"\\nClustering results after refinement:\")\n",
    "    cluster_dict = {}\n",
    "    for idx, label in enumerate(refined_cluster_labels):\n",
    "        if label not in cluster_dict:\n",
    "            cluster_dict[label] = []\n",
    "        cluster_dict[label].append(idx)\n",
    "\n",
    "    for label, indices in cluster_dict.items():\n",
    "        print(f\"\\nCluster {label} contains the following objects:\")\n",
    "        for idx in indices:\n",
    "            print(f\"  Object {idx}: {class_labels_dining[class_labels[idx]]}\")\n",
    "\n",
    "    print(\"\\nDetecting outliers using Elliptic Envelope...\")\n",
    "    features = np.hstack((features_translations, features_sizes))\n",
    "    outliers, decision_scores = detect_outliers_elliptic_envelope(features)\n",
    "    print(f\"\\nElliptic Envelope Outliers: {outliers}\")\n",
    "    print(f\"Decision Scores: {decision_scores}\")\n",
    "\n",
    "    cluster_largest_objects = find_largest_object_in_clusters(features_sizes, refined_cluster_labels, class_labels)\n",
    "\n",
    "    print(\"\\nLargest objects in each cluster by size:\")\n",
    "    for cluster, info in cluster_largest_objects.items():\n",
    "        label_name = class_labels_dining[info['label']]\n",
    "        print(f\"Cluster {cluster}: Largest Object Index {info['index']} with Area {info['area']} and Label '{label_name}'\")\n",
    "\n",
    "    cluster_trees = build_tree(cluster_largest_objects, refined_cluster_labels, features_sizes, class_labels, class_labels_dining)\n",
    "\n",
    "    for cluster, tree in cluster_trees.items():\n",
    "        print(f\"\\nCluster {cluster} Tree Structure:\")\n",
    "        for node, data in tree.items():\n",
    "            print(f\"Root: Object {node} ({data['label']})\")\n",
    "            for child in data[\"children\"]:\n",
    "                print(f\"  â””â”€ Child Object {child['index']} ({child['label']})\")\n",
    "\n",
    "\n",
    "def main_all_scenes():\n",
    "    processed_path = '/home/gaurav/Desktop/furniture_code/3d_front_preprocessed/'\n",
    "    if not os.path.exists(processed_path):\n",
    "        print(f\"Processed path {processed_path} does not exist.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    all_paths = [os.path.join(processed_path, dir_) for dir_ in os.listdir(processed_path)]\n",
    "\n",
    "    all_scenes = []\n",
    "    for path_ in all_paths:\n",
    "        if os.path.isdir(path_):\n",
    "            all_scenes.extend([os.path.join(path_, x) for x in os.listdir(path_) if os.path.isdir(os.path.join(path_, x))])\n",
    "\n",
    "    print(f\"Found {len(all_scenes)} scenes to process.\\n\")\n",
    "\n",
    "    for scene_path in all_scenes:\n",
    "        scene_id = os.path.basename(scene_path)\n",
    "        print(f\"\\n=== Processing Scene: {scene_id} ===\")\n",
    "        try:\n",
    "            main(scene_id)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {scene_id}: {str(e)}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main_all_scenes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Step 1: Select root based on semantic support, excluding same-type objects as children\n",
    "# def find_semantic_root_in_clusters(class_labels, cluster_labels, weights, label_names):\n",
    "#     cluster_roots = {}\n",
    "\n",
    "#     for cluster in np.unique(cluster_labels):\n",
    "#         indices = np.where(cluster_labels == cluster)[0]\n",
    "#         max_support = -1\n",
    "#         root_idx = -1\n",
    "\n",
    "#         for i in indices:\n",
    "#             label_i = label_names[class_labels[i]]\n",
    "#             support = 0\n",
    "#             for j in indices:\n",
    "#                 if i == j:\n",
    "#                     continue\n",
    "#                 label_j = label_names[class_labels[j]]\n",
    "#                 if label_j == label_i:\n",
    "#                     continue  # skip same-type objects\n",
    "#                 support += weights.get(label_i, {}).get(label_j, 0.0)\n",
    "\n",
    "#             if support > max_support:\n",
    "#                 max_support = support\n",
    "#                 root_idx = i\n",
    "\n",
    "#         cluster_roots[cluster] = {\n",
    "#             \"index\": root_idx,\n",
    "#             \"label\": class_labels[root_idx],\n",
    "#             \"support\": max_support\n",
    "#         }\n",
    "\n",
    "#     return cluster_roots\n",
    "\n",
    "# # Step 2: Refine clusters based on weight/distance from semantic root\n",
    "# def refine_clusters_after_estimation(cluster_labels, features_sizes, class_labels, distance_matrix, weights, label_names, threshold=0.4):\n",
    "#     refined_labels = cluster_labels.copy()\n",
    "#     outliers = []\n",
    "#     next_cluster_id = max(refined_labels) + 1\n",
    "\n",
    "#     # Use semantic support-based root selection\n",
    "#     cluster_roots = find_semantic_root_in_clusters(class_labels, cluster_labels, weights, label_names)\n",
    "\n",
    "#     for cluster_id in np.unique(cluster_labels):\n",
    "#         root_info = cluster_roots[cluster_id]\n",
    "#         root_idx = root_info[\"index\"]\n",
    "#         root_label = label_names[class_labels[root_idx]]\n",
    "\n",
    "#         cluster_indices = np.where(refined_labels == cluster_id)[0]\n",
    "\n",
    "#         for idx in cluster_indices:\n",
    "#             if idx == root_idx:\n",
    "#                 continue\n",
    "\n",
    "#             obj_label = label_names[class_labels[idx]]\n",
    "#             if obj_label == root_label:\n",
    "#                 continue  # skip assigning same-type objects as children\n",
    "\n",
    "#             weight = weights.get(root_label, {}).get(obj_label, 0.0)\n",
    "#             distance = distance_matrix[root_idx, idx]\n",
    "#             support = weight / (1 + distance)\n",
    "\n",
    "#             print(f\"[Cluster {cluster_id}] Root: {root_label} ({root_idx}), Object: {obj_label} ({idx})\")\n",
    "#             print(f\"    Weight: {weight}, Distance: {distance:.3f}, Support Score: {support:.3f}\")\n",
    "\n",
    "#             if support < threshold:\n",
    "#                 refined_labels[idx] = next_cluster_id\n",
    "#                 outliers.append(idx)\n",
    "#                 print(f\"    âž¤ OUTLIER â†’ New Cluster {next_cluster_id}\")\n",
    "#                 next_cluster_id += 1\n",
    "#             else:\n",
    "#                 print(f\"    âœ“ KEPT in Cluster {cluster_id}\")\n",
    "\n",
    "#     return refined_labels, outliers\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Select root based on semantic support, excluding same-type objects as children\n",
    "def find_semantic_root_in_clusters(class_labels, cluster_labels, weights, label_names):\n",
    "    cluster_roots = {}\n",
    "\n",
    "    for cluster in np.unique(cluster_labels):\n",
    "        indices = np.where(cluster_labels == cluster)[0]\n",
    "        max_support = -1\n",
    "        root_idx = -1\n",
    "\n",
    "        for i in indices:\n",
    "            label_i = label_names[class_labels[i]]\n",
    "            support = 0\n",
    "            for j in indices:\n",
    "                if i == j:\n",
    "                    continue\n",
    "                label_j = label_names[class_labels[j]]\n",
    "                if label_j == label_i:\n",
    "                    continue  # skip same-type objects\n",
    "                support += weights.get(label_i, {}).get(label_j, 0.0)\n",
    "\n",
    "            if support > max_support:\n",
    "                max_support = support\n",
    "                root_idx = i\n",
    "\n",
    "        cluster_roots[cluster] = {\n",
    "            \"index\": root_idx,\n",
    "            \"label\": class_labels[root_idx],\n",
    "            \"support\": max_support\n",
    "        }\n",
    "\n",
    "    return cluster_roots\n",
    "\n",
    "# Step 2: Refine clusters based on weighted support (prioritizing weight)\n",
    "def refine_clusters_after_estimation(cluster_labels, features_sizes, class_labels, distance_matrix, weights, label_names, threshold=0.6):\n",
    "    refined_labels = cluster_labels.copy()\n",
    "    outliers = []\n",
    "    next_cluster_id = max(refined_labels) + 1\n",
    "\n",
    "    # Use semantic support-based root selection\n",
    "    cluster_roots = find_semantic_root_in_clusters(class_labels, cluster_labels, weights, label_names)\n",
    "\n",
    "    for cluster_id in np.unique(cluster_labels):\n",
    "        root_info = cluster_roots[cluster_id]\n",
    "        root_idx = root_info[\"index\"]\n",
    "        root_label = label_names[class_labels[root_idx]]\n",
    "\n",
    "        cluster_indices = np.where(refined_labels == cluster_id)[0]\n",
    "\n",
    "        for idx in cluster_indices:\n",
    "            if idx == root_idx:\n",
    "                continue\n",
    "\n",
    "            obj_label = label_names[class_labels[idx]]\n",
    "            if obj_label == root_label:\n",
    "                continue  # skip assigning same-type objects as children\n",
    "\n",
    "            weight = weights.get(root_label, {}).get(obj_label, 0.0)\n",
    "            distance = distance_matrix[root_idx, idx]\n",
    "            support = (2.0 * weight) / (1.0 + distance)  # prioritize weight more heavily\n",
    "\n",
    "            print(f\"[Cluster {cluster_id}] Root: {root_label} ({root_idx}), Object: {obj_label} ({idx})\")\n",
    "            print(f\"    Weight: {weight}, Distance: {distance:.3f}, Support Score: {support:.3f}\")\n",
    "\n",
    "            if support < threshold:\n",
    "                refined_labels[idx] = next_cluster_id\n",
    "                outliers.append(idx)\n",
    "                print(f\"    âž¤ OUTLIER â†’ New Cluster {next_cluster_id}\")\n",
    "                next_cluster_id += 1\n",
    "            else:\n",
    "                print(f\"    âœ“ KEPT in Cluster {cluster_id}\")\n",
    "\n",
    "    return refined_labels, outliers\n",
    "\n",
    "\n",
    "def main(scene_id):\n",
    "    processed_path = '/home/gaurav/Desktop/furniture_code/3d_front_preprocessed/'\n",
    "    if not os.path.exists(processed_path):\n",
    "        print(f\"Processed path {processed_path} does not exist.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    all_paths = [os.path.join(processed_path, dir_) for dir_ in os.listdir(processed_path)]\n",
    "\n",
    "    all_scenes = []\n",
    "    for path_ in all_paths:\n",
    "        if os.path.isdir(path_):\n",
    "            all_scenes.extend([os.path.join(path_, x) for x in os.listdir(path_) if os.path.isdir(os.path.join(path_, x))])\n",
    "\n",
    "    scene_path = fetch_scene_id(all_scenes, scene_id)\n",
    "    if scene_path is None:\n",
    "        print(f\"Scene ID {scene_id} not found.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    required_attribs = ['class_labels', 'translations', 'sizes', 'angles']\n",
    "    attribs = load_attributes_from_npz(scene_path, required_attribs)\n",
    "\n",
    "    features_translations, features_sizes = extract_features_from_bboxes(attribs)\n",
    "    total_objects = attribs['class_labels'].shape[0]\n",
    "    print(f\"Total number of objects in the scene: {total_objects}\")\n",
    "\n",
    "    distance_matrix = dist_matrix(features_translations, features_sizes)\n",
    "    cluster_labels = cluster_scene_objects_gmm(distance_matrix, max_components=total_objects)\n",
    "\n",
    "    class_labels = attribs['class_labels'].argmax(-1)\n",
    "\n",
    "    refined_cluster_labels, semantic_outliers = refine_clusters_after_estimation(\n",
    "        cluster_labels,\n",
    "        features_sizes,\n",
    "        class_labels,\n",
    "        distance_matrix,\n",
    "        weights,\n",
    "        class_labels_dining,\n",
    "        threshold=0.1\n",
    "    )\n",
    "\n",
    "\n",
    "    print(\"\\nSemantic-Spatial Outliers (based on affinity score):\", semantic_outliers)\n",
    "\n",
    "    print(\"\\nClass labels for all objects in the scene:\")\n",
    "    for i, label_idx in enumerate(class_labels):\n",
    "        print(f\"Object {i}: {class_labels_dining[label_idx]} (Class Index: {label_idx})\")\n",
    "\n",
    "    print(\"\\nClustering results after refinement:\")\n",
    "    cluster_dict = {}\n",
    "    for idx, label in enumerate(refined_cluster_labels):\n",
    "        if label not in cluster_dict:\n",
    "            cluster_dict[label] = []\n",
    "        cluster_dict[label].append(idx)\n",
    "\n",
    "    for label, indices in cluster_dict.items():\n",
    "        print(f\"\\nCluster {label} contains the following objects:\")\n",
    "        for idx in indices:\n",
    "            print(f\"  Object {idx}: {class_labels_dining[class_labels[idx]]}\")\n",
    "\n",
    "    print(\"\\nDetecting outliers using Elliptic Envelope...\")\n",
    "    features = np.hstack((features_translations, features_sizes))\n",
    "    outliers, decision_scores = detect_outliers_elliptic_envelope(features)\n",
    "    print(f\"\\nElliptic Envelope Outliers: {outliers}\")\n",
    "    print(f\"Decision Scores: {decision_scores}\")\n",
    "\n",
    "    cluster_largest_objects = find_largest_object_in_clusters(features_sizes, refined_cluster_labels, class_labels)\n",
    "\n",
    "    print(\"\\nLargest objects in each cluster by size:\")\n",
    "    for cluster, info in cluster_largest_objects.items():\n",
    "        label_name = class_labels_dining[info['label']]\n",
    "        print(f\"Cluster {cluster}: Largest Object Index {info['index']} with Area {info['area']} and Label '{label_name}'\")\n",
    "\n",
    "    cluster_trees = build_tree(cluster_largest_objects, refined_cluster_labels, features_sizes, class_labels, class_labels_dining)\n",
    "\n",
    "    for cluster, tree in cluster_trees.items():\n",
    "        print(f\"\\nCluster {cluster} Tree Structure:\")\n",
    "        for node, data in tree.items():\n",
    "            print(f\"Root: Object {node} ({data['label']})\")\n",
    "            for child in data[\"children\"]:\n",
    "                print(f\"  â””â”€ Child Object {child['index']} ({child['label']})\")\n",
    "\n",
    "\n",
    "def main_all_scenes():\n",
    "    processed_path = '/home/gaurav/Desktop/furniture_code/3d_front_preprocessed/'\n",
    "    if not os.path.exists(processed_path):\n",
    "        print(f\"Processed path {processed_path} does not exist.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    all_paths = [os.path.join(processed_path, dir_) for dir_ in os.listdir(processed_path)]\n",
    "\n",
    "    all_scenes = []\n",
    "    for path_ in all_paths:\n",
    "        if os.path.isdir(path_):\n",
    "            all_scenes.extend([os.path.join(path_, x) for x in os.listdir(path_) if os.path.isdir(os.path.join(path_, x))])\n",
    "\n",
    "    print(f\"Found {len(all_scenes)} scenes to process.\\n\")\n",
    "\n",
    "    for scene_path in all_scenes:\n",
    "        scene_id = os.path.basename(scene_path)\n",
    "        print(f\"\\n=== Processing Scene: {scene_id} ===\")\n",
    "        try:\n",
    "            main(scene_id)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {scene_id}: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_all_scenes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Step 1: Select root based on semantic support, excluding same-type objects as children\n",
    "def find_semantic_root_in_clusters(class_labels, cluster_labels, weights, label_names):\n",
    "    cluster_roots = {}\n",
    "\n",
    "    for cluster in np.unique(cluster_labels):\n",
    "        indices = np.where(cluster_labels == cluster)[0]\n",
    "        max_support = -1\n",
    "        root_idx = -1\n",
    "\n",
    "        for i in indices:\n",
    "            label_i = label_names[class_labels[i]]\n",
    "            support = 0\n",
    "            for j in indices:\n",
    "                if i == j:\n",
    "                    continue\n",
    "                label_j = label_names[class_labels[j]]\n",
    "                if label_j == label_i:\n",
    "                    continue  # skip same-type objects\n",
    "                support += weights.get(label_i, {}).get(label_j, 0.0)\n",
    "\n",
    "            if support > max_support:\n",
    "                max_support = support\n",
    "                root_idx = i\n",
    "\n",
    "        cluster_roots[cluster] = {\n",
    "            \"index\": root_idx,\n",
    "            \"label\": class_labels[root_idx],\n",
    "            \"support\": max_support\n",
    "        }\n",
    "\n",
    "    return cluster_roots\n",
    "\n",
    "# Step 2: Refine clusters based on weighted support (prioritizing weight)\n",
    "def refine_clusters_after_estimation(cluster_labels, features_sizes, class_labels, distance_matrix, weights, label_names, threshold=0.4):\n",
    "    refined_labels = cluster_labels.copy()\n",
    "    outliers = []\n",
    "    next_cluster_id = max(refined_labels) + 1\n",
    "\n",
    "    # Use semantic support-based root selection\n",
    "    cluster_roots = find_semantic_root_in_clusters(class_labels, cluster_labels, weights, label_names)\n",
    "\n",
    "    for cluster_id in np.unique(cluster_labels):\n",
    "        root_info = cluster_roots[cluster_id]\n",
    "        root_idx = root_info[\"index\"]\n",
    "        root_label = label_names[class_labels[root_idx]]\n",
    "\n",
    "        cluster_indices = np.where(refined_labels == cluster_id)[0]\n",
    "\n",
    "        for idx in cluster_indices:\n",
    "            if idx == root_idx:\n",
    "                continue\n",
    "\n",
    "            obj_label = label_names[class_labels[idx]]\n",
    "            if obj_label == root_label:\n",
    "                continue  # skip assigning same-type objects as children\n",
    "\n",
    "            weight = weights.get(root_label, {}).get(obj_label, 0.0)\n",
    "            distance = distance_matrix[root_idx, idx]\n",
    "            support = (2.0 * weight) / (1.0 + distance)  # prioritize weight more heavily\n",
    "\n",
    "            print(f\"[Cluster {cluster_id}] Root: {root_label} ({root_idx}), Object: {obj_label} ({idx})\")\n",
    "            print(f\"    Weight: {weight}, Distance: {distance:.3f}, Support Score: {support:.3f}\")\n",
    "\n",
    "            if support < threshold:\n",
    "                refined_labels[idx] = next_cluster_id\n",
    "                outliers.append(idx)\n",
    "                print(f\"    âž¤ OUTLIER â†’ New Cluster {next_cluster_id}\")\n",
    "                next_cluster_id += 1\n",
    "            else:\n",
    "                print(f\"    âœ“ KEPT in Cluster {cluster_id}\")\n",
    "\n",
    "    return refined_labels, outliers\n",
    "\n",
    "# Step 3: Merge clusters if root-to-root affinity is high\n",
    "def merge_clusters_by_root_affinity(refined_labels, features_sizes, class_labels, distance_matrix, weights, label_names, merge_threshold=0.4):\n",
    "    cluster_roots = find_semantic_root_in_clusters(class_labels, refined_labels, weights, label_names)\n",
    "    merged_labels = refined_labels.copy()\n",
    "\n",
    "    cluster_ids = list(np.unique(refined_labels))\n",
    "    merged_into = {}  # track merged mappings\n",
    "\n",
    "    for i in range(len(cluster_ids)):\n",
    "        id_a = cluster_ids[i]\n",
    "        root_a = cluster_roots[id_a]['index']\n",
    "        label_a = label_names[class_labels[root_a]]\n",
    "\n",
    "        for j in range(i + 1, len(cluster_ids)):\n",
    "            id_b = cluster_ids[j]\n",
    "            if id_b in merged_into:\n",
    "                continue\n",
    "\n",
    "            root_b = cluster_roots[id_b]['index']\n",
    "            label_b = label_names[class_labels[root_b]]\n",
    "\n",
    "            weight = weights.get(label_a, {}).get(label_b, 0.0)\n",
    "            distance = distance_matrix[root_a, root_b]\n",
    "            support = (2.0 * weight) / (1.0 + distance)\n",
    "\n",
    "            if support > merge_threshold:\n",
    "                print(f\"Merging Cluster {id_b} into Cluster {id_a} â†’ Support: {support:.3f}\")\n",
    "                merged_labels[merged_labels == id_b] = id_a\n",
    "                merged_into[id_b] = id_a\n",
    "\n",
    "    return merged_labels\n",
    "\n",
    "\n",
    "def main(scene_id):\n",
    "    processed_path = '/home/gaurav/Desktop/furniture_code/3d_front_preprocessed/'\n",
    "    if not os.path.exists(processed_path):\n",
    "        print(f\"Processed path {processed_path} does not exist.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    all_paths = [os.path.join(processed_path, dir_) for dir_ in os.listdir(processed_path)]\n",
    "\n",
    "    all_scenes = []\n",
    "    for path_ in all_paths:\n",
    "        if os.path.isdir(path_):\n",
    "            all_scenes.extend([os.path.join(path_, x) for x in os.listdir(path_) if os.path.isdir(os.path.join(path_, x))])\n",
    "\n",
    "    scene_path = fetch_scene_id(all_scenes, scene_id)\n",
    "    if scene_path is None:\n",
    "        print(f\"Scene ID {scene_id} not found.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    required_attribs = ['class_labels', 'translations', 'sizes', 'angles']\n",
    "    attribs = load_attributes_from_npz(scene_path, required_attribs)\n",
    "\n",
    "    features_translations, features_sizes = extract_features_from_bboxes(attribs)\n",
    "    total_objects = attribs['class_labels'].shape[0]\n",
    "    print(f\"Total number of objects in the scene: {total_objects}\")\n",
    "\n",
    "    distance_matrix = dist_matrix(features_translations, features_sizes)\n",
    "    cluster_labels = cluster_scene_objects_gmm(distance_matrix, max_components=total_objects)\n",
    "\n",
    "    class_labels = attribs['class_labels'].argmax(-1)\n",
    "\n",
    "    refined_cluster_labels, semantic_outliers = refine_clusters_after_estimation(\n",
    "        cluster_labels,\n",
    "        features_sizes,\n",
    "        class_labels,\n",
    "        distance_matrix,\n",
    "        weights,\n",
    "        class_labels_dining,\n",
    "        threshold=0.1\n",
    "    )\n",
    "\n",
    "\n",
    "    print(\"\\nSemantic-Spatial Outliers (based on affinity score):\", semantic_outliers)\n",
    "\n",
    "    print(\"\\nClass labels for all objects in the scene:\")\n",
    "    for i, label_idx in enumerate(class_labels):\n",
    "        print(f\"Object {i}: {class_labels_dining[label_idx]} (Class Index: {label_idx})\")\n",
    "\n",
    "    print(\"\\nClustering results after refinement:\")\n",
    "    cluster_dict = {}\n",
    "    for idx, label in enumerate(refined_cluster_labels):\n",
    "        if label not in cluster_dict:\n",
    "            cluster_dict[label] = []\n",
    "        cluster_dict[label].append(idx)\n",
    "\n",
    "    for label, indices in cluster_dict.items():\n",
    "        print(f\"\\nCluster {label} contains the following objects:\")\n",
    "        for idx in indices:\n",
    "            print(f\"  Object {idx}: {class_labels_dining[class_labels[idx]]}\")\n",
    "\n",
    "    print(\"\\nDetecting outliers using Elliptic Envelope...\")\n",
    "    features = np.hstack((features_translations, features_sizes))\n",
    "    outliers, decision_scores = detect_outliers_elliptic_envelope(features)\n",
    "    print(f\"\\nElliptic Envelope Outliers: {outliers}\")\n",
    "    print(f\"Decision Scores: {decision_scores}\")\n",
    "\n",
    "    cluster_largest_objects = find_largest_object_in_clusters(features_sizes, refined_cluster_labels, class_labels)\n",
    "\n",
    "    print(\"\\nLargest objects in each cluster by size:\")\n",
    "    for cluster, info in cluster_largest_objects.items():\n",
    "        label_name = class_labels_dining[info['label']]\n",
    "        print(f\"Cluster {cluster}: Largest Object Index {info['index']} with Area {info['area']} and Label '{label_name}'\")\n",
    "\n",
    "    cluster_trees = build_tree(cluster_largest_objects, refined_cluster_labels, features_sizes, class_labels, class_labels_dining)\n",
    "\n",
    "    for cluster, tree in cluster_trees.items():\n",
    "        print(f\"\\nCluster {cluster} Tree Structure:\")\n",
    "        for node, data in tree.items():\n",
    "            print(f\"Root: Object {node} ({data['label']})\")\n",
    "            for child in data[\"children\"]:\n",
    "                print(f\"  â””â”€ Child Object {child['index']} ({child['label']})\")\n",
    "\n",
    "\n",
    "def main_all_scenes():\n",
    "    processed_path = '/home/gaurav/Desktop/furniture_code/3d_front_preprocessed/'\n",
    "    if not os.path.exists(processed_path):\n",
    "        print(f\"Processed path {processed_path} does not exist.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    all_paths = [os.path.join(processed_path, dir_) for dir_ in os.listdir(processed_path)]\n",
    "\n",
    "    all_scenes = []\n",
    "    for path_ in all_paths:\n",
    "        if os.path.isdir(path_):\n",
    "            all_scenes.extend([os.path.join(path_, x) for x in os.listdir(path_) if os.path.isdir(os.path.join(path_, x))])\n",
    "\n",
    "    print(f\"Found {len(all_scenes)} scenes to process.\\n\")\n",
    "\n",
    "    for scene_path in all_scenes:\n",
    "        scene_id = os.path.basename(scene_path)\n",
    "        print(f\"\\n=== Processing Scene: {scene_id} ===\")\n",
    "        try:\n",
    "            main(scene_id)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {scene_id}: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_all_scenes()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
